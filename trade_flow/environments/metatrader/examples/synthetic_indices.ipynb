{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gymnasium as gym\n",
        "from trade_flow.environments import metatrader\n",
        "\n",
        "from stable_baselines3 import A2C, PPO, DQN\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create Synthetics Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from datetime import datetime\n",
        "from gymnasium.envs.registration import register\n",
        "from trade_flow.environments.metatrader import Simulator, Timeframe, FOREX_DATA_PATH\n",
        "\n",
        "DATA_DIR = os.path.dirname(os.getcwd())\n",
        "SYNTHETIC_INDICES_DATA_PATH = os.path.join(DATA_DIR, \"examples/data/synthetic_indices_symbols.joblib\")\n",
        "SYNTHETIC_INDICES_DATA_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data(symbols: List[str] = [\"EURUSD\", \"GBPCAD\", \"USDJPY\"], \n",
        "             time_range: Tuple[datetime, datetime] = (datetime(2011, 1, 1), datetime(2012, 12, 31)),\n",
        "             timeframe: Timeframe = Timeframe.D1, \n",
        "             filename: str = FOREX_DATA_PATH):\n",
        "    \n",
        "    mt_sim = metatrader.Simulator()\n",
        "    mt_sim.download_data(symbols, time_range, timeframe)\n",
        "    mt_sim.save_symbols(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the time range for the data download\n",
        "start_date = datetime(2020, 1, 1)\n",
        "end_date = datetime(2023, 12, 31)\n",
        "time_range = (start_date, end_date)\n",
        "\n",
        "# synthetic indices\n",
        "\n",
        "# synthetic_indices_symbols = [\n",
        "#     \"Volatility 10 Index\", \n",
        "#     \"Volatility 25 Index\",  \n",
        "#     \"Volatility 75 (1s) Index\",\n",
        "#     \"Volatility 150 (1s) Index\",\n",
        "#     \"Volatility 200 (1s) Index\",\n",
        "#     \"Volatility 250 (1s) Index\"]\n",
        "synthetic_indices_symbols = [\n",
        "    \"Step Index\",\n",
        "    \"Volatility 25 Index\",  \n",
        "    \"Volatility 75 (1s) Index\",\n",
        "    \"Volatility 150 (1s) Index\",]\n",
        "get_data(synthetic_indices_symbols, time_range, Timeframe.H4, SYNTHETIC_INDICES_DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "register(\n",
        "    id=\"synthetic-indices-hedge-v0\",\n",
        "    entry_point=\"trade_flow.environments.metatrader.envs:MT5Env\",\n",
        "    kwargs={\n",
        "        \"original_simulator\": Simulator(balance=1000, leverage=500, symbols_filename=SYNTHETIC_INDICES_DATA_PATH, hedge=True),\n",
        "        \"trading_symbols\": synthetic_indices_symbols,\n",
        "        \"window_size\": 10,\n",
        "        \"symbol_max_orders\": 3,\n",
        "        \"hold_threshold\": 0.3,\n",
        "        \"close_threshold\": 0.7,\n",
        "        \"fee\": 0.1,\n",
        "    },\n",
        ")\n",
        "\n",
        "register(\n",
        "    id=\"synthetic-indices-unhedge-v0\",\n",
        "    entry_point=\"trade_flow.environments.metatrader.envs:MT5Env\",\n",
        "    kwargs={\n",
        "        \"original_simulator\": Simulator(balance=1000, leverage=500, symbols_filename=SYNTHETIC_INDICES_DATA_PATH, hedge=False),\n",
        "        \"trading_symbols\": synthetic_indices_symbols,\n",
        "        \"window_size\": 10,\n",
        "        \"hold_threshold\": 0.5,\n",
        "        \"close_threshold\": 0.5,\n",
        "        \"fee\": 0.1,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# env_name = 'forex-hedge-v0'\n",
        "# env_name = 'stocks-hedge-v0'\n",
        "# env_name = 'crypto-hedge-v0'\n",
        "# env_name = 'mixed-hedge-v0'\n",
        "\n",
        "# env_name = 'forex-unhedge-v0'\n",
        "# env_name = 'stocks-unhedge-v0'\n",
        "# env_name = 'crypto-unhedge-v0'\n",
        "# env_name = 'mixed-unhedge-v0'\n",
        "\n",
        "env_name = 'synthetic-indices-hedge-v0'\n",
        "# env_name = 'synthetic-indices-unhedge-v0'\n",
        "\n",
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(reward_over_episodes):\n",
        "    \"\"\"  Print Reward  \"\"\"\n",
        "\n",
        "    avg = np.mean(reward_over_episodes)\n",
        "    min = np.min(reward_over_episodes)\n",
        "    max = np.max(reward_over_episodes)\n",
        "\n",
        "    print (f'Min. Reward          : {min:>10.3f}')\n",
        "    print (f'Avg. Reward          : {avg:>10.3f}')\n",
        "    print (f'Max. Reward          : {max:>10.3f}')\n",
        "\n",
        "    return min, avg, max\n",
        "\n",
        "\n",
        "# ProgressBarCallback for model.learn()\n",
        "class ProgressBarCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq: int, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        self.progress_bar = tqdm(total=self.model._total_timesteps, desc=\"model.learn()\")\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            self.progress_bar.update(self.check_freq)\n",
        "        return True\n",
        "    \n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        self.progress_bar.close()\n",
        "\n",
        "\n",
        "# TRAINING + TEST\n",
        "def train_test_model(model, env, seed, total_num_episodes, total_learning_timesteps=10_000):\n",
        "    \"\"\" if model=None then execute 'Random actions' \"\"\"\n",
        "\n",
        "    # reproduce training and test\n",
        "    print('-' * 80)\n",
        "    obs = env.reset(seed=seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    vec_env = None\n",
        "\n",
        "    if model is not None:\n",
        "        print(f'model {type(model)}')\n",
        "        print(f'policy {type(model.policy)}')\n",
        "        # print(f'model.learn(): {total_learning_timesteps} timesteps ...')\n",
        "\n",
        "        # custom callback for 'progress_bar'\n",
        "        model.learn(total_timesteps=total_learning_timesteps, callback=ProgressBarCallback(100))\n",
        "        # model.learn(total_timesteps=total_learning_timesteps, progress_bar=True)\n",
        "        # ImportError: You must install tqdm and rich in order to use the progress bar callback. \n",
        "        # It is included if you install stable-baselines with the extra packages: `pip install stable-baselines3[extra]`\n",
        "\n",
        "        vec_env = model.get_env()\n",
        "        obs = vec_env.reset()\n",
        "    else:\n",
        "        print (\"RANDOM actions\")\n",
        "\n",
        "    reward_over_episodes = []\n",
        "\n",
        "    tbar = tqdm(range(total_num_episodes))\n",
        "\n",
        "    for episode in tbar:\n",
        "        \n",
        "        if vec_env: \n",
        "            obs = vec_env.reset()\n",
        "        else:\n",
        "            obs, info = env.reset()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if model is not None:\n",
        "                action, _states = model.predict(obs)\n",
        "                obs, reward, done, info = vec_env.step(action)\n",
        "            else: # random\n",
        "                action = env.action_space.sample()\n",
        "                obs, reward, terminated, truncated, info = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        reward_over_episodes.append(total_reward)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            avg_reward = np.mean(reward_over_episodes)\n",
        "            tbar.set_description(f'Episode: {episode}, Avg. Reward: {avg_reward:.3f}')\n",
        "            tbar.update()\n",
        "\n",
        "    tbar.close()\n",
        "    avg_reward = np.mean(reward_over_episodes)\n",
        "\n",
        "    return reward_over_episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train + Test Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 2024  # random seed\n",
        "total_num_episodes = 50\n",
        "\n",
        "print (\"env_name                 :\", env_name)\n",
        "print (\"seed                     :\", seed)\n",
        "\n",
        "# INIT matplotlib\n",
        "plot_settings = {}\n",
        "plot_data = {'x': [i for i in range(1, total_num_episodes + 1)]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random actions\n",
        "model = None \n",
        "total_learning_timesteps = 0\n",
        "rewards = train_test_model(model, env, seed, total_num_episodes, total_learning_timesteps)\n",
        "min, avg, max = print_stats(rewards)\n",
        "class_name = f'Random actions'\n",
        "label = f'Avg. {avg:>7.2f} : {class_name}'\n",
        "plot_data['rnd_rewards'] = rewards\n",
        "plot_settings['rnd_rewards'] = {'label': label}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SB3 Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "learning_timesteps_list_in_K = [25]\n",
        "# learning_timesteps_list_in_K = [50, 250, 500]\n",
        "# learning_timesteps_list_in_K = [500, 1000, 3000, 5000]\n",
        "\n",
        "# RL Algorithms: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
        "model_class_list = [A2C, PPO, DQN]\n",
        "\n",
        "for timesteps in learning_timesteps_list_in_K:\n",
        "    total_learning_timesteps = timesteps * 1000\n",
        "    step_key = f'{timesteps}K'\n",
        "\n",
        "    for model_class in model_class_list:\n",
        "        policy_dict = model_class.policy_aliases\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
        "        policy = policy_dict.get('MultiInputPolicy') # Try MlpPolicy or MlpLstmPolicy\n",
        "\n",
        "        try:\n",
        "            model = model_class(policy, env, verbose=0)\n",
        "            class_name = type(model).__qualname__\n",
        "            plot_key = f'{class_name}_rewards_'+step_key\n",
        "            rewards = train_test_model(model, env, seed, total_num_episodes, total_learning_timesteps)\n",
        "            min, avg, max, = print_stats(rewards)\n",
        "            label = f'Avg. {avg:>7.2f} : {class_name} - {step_key}'\n",
        "            plot_data[plot_key] = rewards\n",
        "            plot_settings[plot_key] = {'label': label}     \n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {str(e)}\")\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.DataFrame(plot_data)\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for key in plot_data:\n",
        "    if key == 'x':\n",
        "        continue\n",
        "    label = plot_settings[key]['label']\n",
        "    line = plt.plot('x', key, data=data, linewidth=1, label=label)\n",
        "\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('reward')\n",
        "plt.title('Random vs. SB3 Agents')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "p3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "algo_trading",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
