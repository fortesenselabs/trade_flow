{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from stable_baselines3 import A2C, PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fortesenselabs/Tech/labs/Financial_Eng/Financial_Markets/lab/trade_flow/trade_flow/feed/__init__.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df = pd.read_csv(path, parse_dates=True, index_col=index_name)\n"
          ]
        }
      ],
      "source": [
        "from trade_flow.environments import metatrader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fortesenselabs/anaconda3/envs/algo_trading/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (10, 6)\u001b[0m\n",
            "  logger.warn(\n",
            "/home/fortesenselabs/anaconda3/envs/algo_trading/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:29: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: float64. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
            "  logger.warn(\n",
            "/home/fortesenselabs/anaconda3/envs/algo_trading/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:34: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the lower and upper bounds are not [0, 255]. Actual lower bound: -10000000000.0, upper bound: 10000000000.0. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "# env_name = 'forex-hedge-v0'\n",
        "# env_name = 'stocks-hedge-v0'\n",
        "env_name = 'crypto-hedge-v0'\n",
        "# env_name = 'mixed-hedge-v0'\n",
        "\n",
        "# env_name = 'forex-unhedge-v0'\n",
        "# env_name = 'stocks-unhedge-v0'\n",
        "# env_name = 'crypto-unhedge-v0'\n",
        "# env_name = 'mixed-unhedge-v0'\n",
        "\n",
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(reward_over_episodes):\n",
        "    \"\"\"  Print Reward  \"\"\"\n",
        "\n",
        "    avg = np.mean(reward_over_episodes)\n",
        "    min = np.min(reward_over_episodes)\n",
        "    max = np.max(reward_over_episodes)\n",
        "\n",
        "    print (f'Min. Reward          : {min:>10.3f}')\n",
        "    print (f'Avg. Reward          : {avg:>10.3f}')\n",
        "    print (f'Max. Reward          : {max:>10.3f}')\n",
        "\n",
        "    return min, avg, max\n",
        "\n",
        "\n",
        "# ProgressBarCallback for model.learn()\n",
        "class ProgressBarCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq: int, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        self.progress_bar = tqdm(total=self.model._total_timesteps, desc=\"model.learn()\")\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            self.progress_bar.update(self.check_freq)\n",
        "        return True\n",
        "    \n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        self.progress_bar.close()\n",
        "\n",
        "\n",
        "# TRAINING + TEST\n",
        "def train_test_model(model, env, seed, total_num_episodes, total_learning_timesteps=10_000):\n",
        "    \"\"\" if model=None then execute 'Random actions' \"\"\"\n",
        "\n",
        "    # reproduce training and test\n",
        "    print('-' * 80)\n",
        "    obs = env.reset(seed=seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    vec_env = None\n",
        "\n",
        "    if model is not None:\n",
        "        print(f'model {type(model)}')\n",
        "        print(f'policy {type(model.policy)}')\n",
        "        # print(f'model.learn(): {total_learning_timesteps} timesteps ...')\n",
        "\n",
        "        # custom callback for 'progress_bar'\n",
        "        model.learn(total_timesteps=total_learning_timesteps, callback=ProgressBarCallback(100))\n",
        "        # model.learn(total_timesteps=total_learning_timesteps, progress_bar=True)\n",
        "        # ImportError: You must install tqdm and rich in order to use the progress bar callback. \n",
        "        # It is included if you install stable-baselines with the extra packages: `pip install stable-baselines3[extra]`\n",
        "\n",
        "        vec_env = model.get_env()\n",
        "        obs = vec_env.reset()\n",
        "    else:\n",
        "        print (\"RANDOM actions\")\n",
        "\n",
        "    reward_over_episodes = []\n",
        "\n",
        "    tbar = tqdm(range(total_num_episodes))\n",
        "\n",
        "    for episode in tbar:\n",
        "        \n",
        "        if vec_env: \n",
        "            obs = vec_env.reset()\n",
        "        else:\n",
        "            obs, info = env.reset()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if model is not None:\n",
        "                action, _states = model.predict(obs)\n",
        "                obs, reward, done, info = vec_env.step(action)\n",
        "            else: # random\n",
        "                action = env.action_space.sample()\n",
        "                obs, reward, terminated, truncated, info = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        reward_over_episodes.append(total_reward)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            avg_reward = np.mean(reward_over_episodes)\n",
        "            tbar.set_description(f'Episode: {episode}, Avg. Reward: {avg_reward:.3f}')\n",
        "            tbar.update()\n",
        "\n",
        "    tbar.close()\n",
        "    avg_reward = np.mean(reward_over_episodes)\n",
        "\n",
        "    return reward_over_episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train + Test Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env_name                 : crypto-hedge-v0\n",
            "seed                     : 2024\n",
            "--------------------------------------------------------------------------------\n",
            "RANDOM actions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode: 40, Avg. Reward: -9999.744: 100%|██████████| 50/50 [01:09<00:00,  1.39s/it]\n",
            "/home/fortesenselabs/anaconda3/envs/algo_trading/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min. Reward          : -10000.000\n",
            "Avg. Reward          :  -9999.741\n",
            "Max. Reward          :  -9996.443\n",
            "--------------------------------------------------------------------------------\n",
            "model <class 'stable_baselines3.a2c.a2c.A2C'>\n",
            "policy <class 'stable_baselines3.common.policies.MultiInputActorCriticPolicy'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.learn(): 100%|██████████| 25000/25000 [01:29<00:00, 278.21it/s]\n",
            "Episode: 40, Avg. Reward: 176.217: 100%|██████████| 50/50 [01:58<00:00,  2.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min. Reward          :  -3126.357\n",
            "Avg. Reward          :    130.411\n",
            "Max. Reward          :   1834.719\n",
            "--------------------------------------------------------------------------------\n",
            "model <class 'stable_baselines3.ppo.ppo.PPO'>\n",
            "policy <class 'stable_baselines3.common.policies.MultiInputActorCriticPolicy'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.learn():  16%|█▌        | 4000/25000 [00:12<01:04, 325.21it/s]"
          ]
        }
      ],
      "source": [
        "seed = 2024  # random seed\n",
        "total_num_episodes = 50\n",
        "\n",
        "print (\"env_name                 :\", env_name)\n",
        "print (\"seed                     :\", seed)\n",
        "\n",
        "# INIT matplotlib\n",
        "plot_settings = {}\n",
        "plot_data = {'x': [i for i in range(1, total_num_episodes + 1)]}\n",
        "\n",
        "# Random actions\n",
        "model = None \n",
        "total_learning_timesteps = 0\n",
        "rewards = train_test_model(model, env, seed, total_num_episodes, total_learning_timesteps)\n",
        "min, avg, max = print_stats(rewards)\n",
        "class_name = f'Random actions'\n",
        "label = f'Avg. {avg:>7.2f} : {class_name}'\n",
        "plot_data['rnd_rewards'] = rewards\n",
        "plot_settings['rnd_rewards'] = {'label': label}\n",
        "\n",
        "learning_timesteps_list_in_K = [25]\n",
        "# learning_timesteps_list_in_K = [50, 250, 500]\n",
        "# learning_timesteps_list_in_K = [500, 1000, 3000, 5000]\n",
        "\n",
        "# RL Algorithms: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
        "model_class_list = [A2C, PPO]\n",
        "\n",
        "for timesteps in learning_timesteps_list_in_K:\n",
        "    total_learning_timesteps = timesteps * 1000\n",
        "    step_key = f'{timesteps}K'\n",
        "\n",
        "    for model_class in model_class_list:\n",
        "        policy_dict = model_class.policy_aliases\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
        "        policy = policy_dict.get('MultiInputPolicy')\n",
        "\n",
        "        try:\n",
        "            model = model_class(policy, env, verbose=0)\n",
        "            class_name = type(model).__qualname__\n",
        "            plot_key = f'{class_name}_rewards_'+step_key\n",
        "            rewards = train_test_model(model, env, seed, total_num_episodes, total_learning_timesteps)\n",
        "            min, avg, max, = print_stats(rewards)\n",
        "            label = f'Avg. {avg:>7.2f} : {class_name} - {step_key}'\n",
        "            plot_data[plot_key] = rewards\n",
        "            plot_settings[plot_key] = {'label': label}     \n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {str(e)}\")\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.DataFrame(plot_data)\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for key in plot_data:\n",
        "    if key == 'x':\n",
        "        continue\n",
        "    label = plot_settings[key]['label']\n",
        "    line = plt.plot('x', key, data=data, linewidth=1, label=label)\n",
        "\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('reward')\n",
        "plt.title('Random vs. SB3 Agents')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "p3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "algo_trading",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
