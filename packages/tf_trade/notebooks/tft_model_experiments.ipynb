{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFTrade Model Experiments \n",
    "\n",
    "\n",
    "__Financial Prediction research__\n",
    "\n",
    "\n",
    "Research about financial prediction on markets in MetaTrader 5.\n",
    "Deriv Synthetic instruments are currently used as the primary instruments.\n",
    "\n",
    "**NOTE:** Docker is required to run this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas-ta yfinance docker python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start MetaTrader 5 Docker Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Optional\n",
    "from trade_flow.common.logging import Logger\n",
    "from packages.mt5any import (\n",
    "    DockerizedMT5TerminalConfig,\n",
    "    DockerizedMT5Terminal,\n",
    ")\n",
    "from packages.mt5any import MetaTrader5\n",
    "\n",
    "\n",
    "class MT5Exception(Exception):\n",
    "    \"\"\"Base exception class for MT5 errors.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class MT5InitializationError(MT5Exception):\n",
    "    \"\"\"Raised when there is an error initializing MT5 Trader.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class MT5:\n",
    "    \"\"\"\n",
    "    A class to interface with MetaTrader 5 for trading operations using Dockerized MT5 Terminals.\n",
    "\n",
    "    Attributes:\n",
    "        mt5_account_number (str): The MetaTrader 5 account number.\n",
    "        mt5_password (str): The password for the MT5 account.\n",
    "        mt5_server (str): The MT5 server name.\n",
    "        logger (Logger): Logger instance for logging events.\n",
    "        mt5_terminal (DockerizedMT5Terminal): Dockerized MT5 Terminal instance.\n",
    "        mt5 (MetaTrader5): MetaTrader 5 client interface.\n",
    "        initial_balance (float): The initial account balance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        account_number: str,\n",
    "        password: str,\n",
    "        server: str,\n",
    "        logger: Optional[Logger] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MT5 class with account credentials and configurations.\n",
    "\n",
    "        Args:\n",
    "            account_number (str): MetaTrader 5 account number.\n",
    "            password (str): MetaTrader 5 account password.\n",
    "            server (str): MetaTrader 5 server.\n",
    "            logger (Optional[Logger]): Logger instance for logging.\n",
    "        \"\"\"\n",
    "        self.mt5_account_number = account_number\n",
    "        self.mt5_password = password\n",
    "        self.mt5_server = server\n",
    "\n",
    "        # Set up logging\n",
    "        self.logger = logger or Logger(name=\"tft_logger\", log_level=logging.DEBUG, filename=\"TFT_model_experiments.log\")\n",
    "\n",
    "        # Set up MetaTrader 5 terminal and configuration\n",
    "        self.mt5_config = DockerizedMT5TerminalConfig(\n",
    "            account_number=self.mt5_account_number,\n",
    "            password=self.mt5_password,\n",
    "            server=self.mt5_server,\n",
    "            read_only_api=True,\n",
    "        )\n",
    "\n",
    "        # Initialize Dockerized MT5 Terminal\n",
    "        self.mt5_terminal = DockerizedMT5Terminal(config=self.mt5_config)\n",
    "        self._initialize_terminal()\n",
    "\n",
    "        # Initialize MetaTrader 5\n",
    "        self.mt5 = MetaTrader5()\n",
    "        self._initialize_mt5()\n",
    "        self.logger.debug(f\"Terminal Info: {self.mt5.terminal_info()._asdict()}\")\n",
    "\n",
    "        # Get account information\n",
    "        self.account_info = self.mt5.account_info()._asdict()\n",
    "        self.logger.debug(f\"Account Info: {self.account_info}\")\n",
    "\n",
    "        self.initial_balance = self.account_info[\"balance\"]\n",
    "\n",
    "        # Log account info\n",
    "        self.logger.info(f\"Account Balance: {self.initial_balance}\")\n",
    "        self.logger.info(f\"Equity: {self.account_info['equity']}\")\n",
    "        self.logger.info(f\"Currency: {self.account_info['currency']}\")\n",
    "        self.logger.info(f\"Margin: {self.account_info['margin']}\")\n",
    "        self.logger.info(f\"Server: {self.account_info['server']}\")\n",
    "        self.logger.info(f\"Name: {self.account_info['name']}\")\n",
    "\n",
    "    def _initialize_terminal(self) -> None:\n",
    "        \"\"\"Initialize and safely start the Dockerized MT5 Terminal.\"\"\"\n",
    "        try:\n",
    "            self.mt5_terminal.safe_start()\n",
    "            time.sleep(5)\n",
    "\n",
    "            self.logger.info(f\"MetaTrader 5 Terminal started for account {self.mt5_account_number}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing Dockerized MT5 Terminal: {e}\")\n",
    "            raise MT5InitializationError(\"Failed to start Dockerized MT5 Terminal\")\n",
    "\n",
    "    def _initialize_mt5(self) -> None:\n",
    "        \"\"\"Initialize the MetaTrader 5 client.\"\"\"\n",
    "        try:\n",
    "            if not self.mt5.initialize():\n",
    "                raise RuntimeError(\"MetaTrader 5 initialization failed\")\n",
    "\n",
    "            # if not self.mt5.login(self.mt5_account_number, self.mt5_password, self.mt5_server):\n",
    "            #     raise RuntimeError(\"MetaTrader 5 login failed\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing MetaTrader 5: {e}\")\n",
    "            raise MT5InitializationError(\"Failed to initialize MetaTrader 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mt5_venue = MT5(\n",
    "    account_number=os.getenv(\"MT5_ACCOUNT_NUMBER\"),\n",
    "    password=os.getenv(\"MT5_PASSWORD\"),\n",
    "    server=os.getenv(\"MT5_SERVER\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt5_venue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base import\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import matplotlib as matplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "# import jtplot module in notebook\n",
    "# from jupyterthemes import jtplot\n",
    "\n",
    "# choose which theme to inherit plotting style from\n",
    "# onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "# jtplot.style(theme='solarizedd')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.available\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)          # Set seed for Python's random module\n",
    "    np.random.seed(seed)       # Set seed for NumPy's random module\n",
    "    torch.manual_seed(seed)    # Set seed for PyTorch\n",
    "    torch.cuda.manual_seed(seed)  # Set seed for CUDA if using GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for cuDNN\n",
    "    torch.backends.cudnn.benchmark = False      # Disable the benchmark for deterministic results\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Version and Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Library version:\")\n",
    "print(\"\\t-  pandas: {}\".format(pd.__version__))\n",
    "print(\"\\t-  numpy: {}\".format(np.__version__))\n",
    "print(\"\\t-  sklearn: {}\".format(sk.__version__))\n",
    "print(\"\\t-  torch: {}\".format(torch.__version__))\n",
    "print(\"\\t-  matplotlib: {}\".format(matplot.__version__))\n",
    "print(\"\\t-  python: {}\".format(sys.version[:sys.version.find('(') - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df.ta.indicators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ta.sma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MetaTrader 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt5 = mt5_venue.mt5\n",
    "\n",
    "# INITIALIZE THE DEVICE\n",
    "mt5.initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch All Available Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists\n",
    "symbols = []\n",
    "sectors = []\n",
    "descriptions = []\n",
    "\n",
    "# Get the information for all symbol\n",
    "symbols_information = mt5.symbols_get()\n",
    "\n",
    "symbols_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(symbols_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Symbols Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [element[-3] for element in symbols_information]\n",
    "sectors = [element[-1].split(\"\\\\\")[0] for element in symbols_information]\n",
    "descriptions = [element[-7] for element in symbols_information]\n",
    "\n",
    "# Create a dataframe directly from the extracted lists\n",
    "informations = pd.DataFrame({\n",
    "    \"Symbol\": symbols,\n",
    "    \"Sector\": sectors,\n",
    "    \"Description\": descriptions\n",
    "})\n",
    "\n",
    "informations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Forex Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_Informations = informations[~informations['Sector'].str.contains(\"Forex\")]\n",
    "\n",
    "filtered_Informations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take some Volatility pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'Symbol' does not contains 'Volatility'\n",
    "# filter_volatility_pairs = informations[informations['Symbol'].str.contains(\"Volatility\")]\n",
    "\n",
    "# filter_volatility_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Symbols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_df = pd.concat([lowest_spread_asset, filter_volatility_pairs])\n",
    "selected_df = filtered_Informations\n",
    "\n",
    "selected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the symbols on MT5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the selected symbols and select them in MetaTrader5\n",
    "for symbol in selected_df['Symbol']:\n",
    "    # Try to select the symbol, raise an error if it fails\n",
    "    if not mt5.symbol_select(symbol, True):\n",
    "        raise ValueError(f\"Failed to enable symbol: {symbol}\")\n",
    "    else:\n",
    "        print(f\"Symbol enabled: {symbol}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Get data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_data(symbol: str, n: int, timeframe=mt5.TIMEFRAME_D1, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch historical data for a given symbol from MetaTrader 5.\n",
    "\n",
    "    This function retrieves data for a specified symbol either for a certain range of time \n",
    "    or a specific number of bars. It ensures that the symbol is enabled in MetaTrader 5 and \n",
    "    handles both custom time ranges and default time periods.\n",
    "\n",
    "    Args:\n",
    "        symbol (str): The trading symbol (e.g., 'EURUSD').\n",
    "        n (int): Number of bars (data points) to retrieve if not using a time range.\n",
    "        timeframe (int): The time frame to use (e.g., mt5.TIMEFRAME_D1 for daily). Default is mt5.TIMEFRAME_D1.\n",
    "        **kwargs: Additional arguments:\n",
    "            - is_range (bool): Whether to fetch data for a specific time range.\n",
    "            - utc_from (datetime): Start time (UTC) for the time range (default: 2020-01-10).\n",
    "            - utc_to (datetime): End time (UTC) for the time range (default: 2020-01-11 13:00).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing historical rates with 'time' as the index.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the symbol cannot be enabled in MetaTrader 5.\n",
    "\n",
    "    Example Usage:\n",
    "        - To fetch 1000 daily bars:\n",
    "            >>> df = get_data('EURUSD', 1000)\n",
    "\n",
    "        - To fetch data within a specific time range:\n",
    "            >>> df = get_data('EURUSD', 0, is_range=True, utc_from=datetime(2022, 1, 1), utc_to=datetime(2022, 1, 10))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize MetaTrader 5 terminal\n",
    "    mt5.initialize()\n",
    "\n",
    "    # Select symbol and ensure visibility\n",
    "    if not mt5.symbol_select(symbol, True):\n",
    "        raise ValueError(f\"Failed to enable symbol: {symbol}\")\n",
    "\n",
    "    # Handle data retrieval based on range or last 'n' bars\n",
    "    is_range = kwargs.get(\"is_range\", False)\n",
    "    timezone = pytz.timezone(\"Etc/UTC\")  # UTC timezone\n",
    "\n",
    "    if is_range:\n",
    "        # Fetch data in a specific time range\n",
    "        utc_from = kwargs.get(\"utc_from\", datetime(2020, 1, 10, tzinfo=timezone))\n",
    "        utc_to = kwargs.get(\"utc_to\", datetime(2020, 1, 11, hour=13, tzinfo=timezone))\n",
    "        rates = mt5.copy_rates_range(symbol, timeframe, utc_from, utc_to)\n",
    "    else:\n",
    "        # Fetch the last 'n' bars\n",
    "        utc_from = datetime.now() + timedelta(hours=2)\n",
    "        rates = mt5.copy_rates_from(symbol, timeframe, utc_from, n)\n",
    "\n",
    "    # Convert the rates into a DataFrame\n",
    "    rates_frame = pd.DataFrame(rates)\n",
    "\n",
    "    # Convert 'time' from seconds to datetime format and set as index\n",
    "    rates_frame['time'] = pd.to_datetime(rates_frame['time'], unit='s')\n",
    "    # rates_frame['time'] = pd.to_datetime(rates_frame['time'], format='%Y-%m-%d')\n",
    "    rates_frame.set_index('time', inplace=True)\n",
    "\n",
    "    return rates_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Best Spread Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the spread values\n",
    "spread = []\n",
    "\n",
    "# Compute the spread using a list comprehension for efficiency\n",
    "spread = [\n",
    "    (tick.ask - tick.bid) / tick.bid if (tick := mt5.symbol_info_tick(symbol)) and tick.ask and tick.bid else None\n",
    "    for symbol in selected_df[\"Symbol\"]\n",
    "]\n",
    "\n",
    "spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_spread = min([x for x in spread if x is not None])\n",
    "\n",
    "min_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_spread = max([x for x in spread if x is not None])\n",
    "\n",
    "max_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_spread = np.mean([min_spread, max_spread])\n",
    "\n",
    "mean_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_spread/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mean_spread < 0.01:\n",
    "#     spread_threshold = np.round(mean_spread * 100, 4) # 0.0035\n",
    "# else:\n",
    "#     spread_threshold = mean_spread\n",
    "\n",
    "spread_threshold = np.round(mean_spread / 1000, 4) # 0.0035\n",
    "spread_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take the assets with low spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df[\"Spread\"] = spread\n",
    "\n",
    "# Take the assets with the spread < spread_threshold(%)\n",
    "lowest_spread_asset = selected_df.dropna().loc[selected_df[\"Spread\"]<spread_threshold]\n",
    "lowest_spread_asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Final Assets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_assets = lowest_spread_asset\n",
    "# final_assets = lowest_spread_asset.head(10)\n",
    "# Fill Assets with no spread with max spread value\n",
    "# final_assets['Spread'] = final_assets['Spread'].fillna(max_spread) \n",
    "\n",
    "final_assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features engineering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Normalization Scheme\n",
    "\n",
    "This class apply logarithm and a normalization(MinMax | StandarScale | Normalizer_l1 | Normalizer_l2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer as SklearnNormalizer\n",
    "\n",
    "class Normalizer:\n",
    "    \"\"\"\n",
    "    A class for normalizing market data while preserving the original data.\n",
    "\n",
    "    This class applies logarithm and a normalization (MinMax, StandardScale, Normalizer_l1, Normalizer_l2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the Normalizer class with market data.\n",
    "\n",
    "        Args:\n",
    "            data (list[dict]): A list of dictionaries, where each dictionary represents a market\n",
    "                               with keys like \"name\" and \"data\" (containing market data).\n",
    "        \"\"\"\n",
    "        self.__market_names = [e[\"name\"] for e in data]\n",
    "        self.__original_data = [pd.DataFrame(e[\"data\"]) for e in data]  # Convert to DataFrame\n",
    "        self.__normalized_data = None  # Stores normalized data after fit is called\n",
    "\n",
    "    def __normalize(self, target, numerical_data, scaler_func):\n",
    "        \"\"\"\n",
    "        Normalizes data using the provided scaler function for specific target markets.\n",
    "\n",
    "        Args:\n",
    "            target (str): The target market(s) to normalize (\"all\" or a list of market names).\n",
    "            numerical_data (list[pd.DataFrame]): A list of DataFrames containing numerical data for each market.\n",
    "            scaler_func (callable): A function that performs data scaling (e.g., MinMaxScaler.fit_transform).\n",
    "\n",
    "        Returns:\n",
    "            list[pd.DataFrame]: List of normalized market DataFrames.\n",
    "        \"\"\"\n",
    "        normalized_markets = []\n",
    "        target_markets = self.__market_names if target == \"all\" else target\n",
    "\n",
    "        for market_name, market in zip(self.__market_names, numerical_data):\n",
    "            if market_name in target_markets:\n",
    "                columns = market.columns\n",
    "                market_scaled = scaler_func(market)\n",
    "                normalized_markets.append(pd.DataFrame(market_scaled, columns=columns))\n",
    "\n",
    "        return normalized_markets\n",
    "\n",
    "    def fit(self, norm_type, features_list, target=\"all\"):\n",
    "        \"\"\"\n",
    "        Performs data normalization based on the specified type, features, and target market(s).\n",
    "\n",
    "        Args:\n",
    "            norm_type (str): The type of normalization to perform (\"MinMax\", \"StandardScale\", \"Normalizer_l1\", or \"Normalizer_l2\").\n",
    "            features_list (list): A list of feature names to consider for normalization.\n",
    "            target (str, optional): The target market(s) to normalize (\"all\" or a list of market names). Defaults to \"all\".\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an invalid normalization type is provided.\n",
    "        \"\"\"\n",
    "        # Extract numerical data for the specified features\n",
    "        numerical_data = [market[features_list]._get_numeric_data() for market in self.__original_data]\n",
    "\n",
    "        if norm_type == \"MinMax\":\n",
    "            print(f\"Performing MinMax Normalization on {target}.\")\n",
    "            scaler_func = MinMaxScaler().fit_transform\n",
    "        elif norm_type == \"StandardScale\":\n",
    "            print(f\"Performing StandardScale Normalization on {target}.\")\n",
    "            scaler_func = StandardScaler().fit_transform\n",
    "        elif norm_type.startswith(\"Normalizer\"):\n",
    "            norm_value = norm_type.split(\"_\")[-1]  # Extract l1 or l2 from \"Normalizer_l1\" or \"Normalizer_l2\"\n",
    "            if norm_value not in (\"l1\", \"l2\"):\n",
    "                raise ValueError(\"Invalid norm type for Normalizer. Must be 'l1' or 'l2'.\")\n",
    "            print(f\"Performing Normalizer (norm={norm_value}) on {target}.\")\n",
    "            scaler_func = SklearnNormalizer(norm=norm_value).fit_transform\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid normalization type: {norm_type}\")\n",
    "\n",
    "        self.__normalized_data = self.__normalize(target, numerical_data, scaler_func)\n",
    "\n",
    "    def get_normalized_data(self, idx: int = None):\n",
    "        \"\"\"\n",
    "        Returns the normalized data if normalization has been performed, otherwise raises an error.\n",
    "        \n",
    "        Args:\n",
    "            idx (int, optional): The location/index of the normalized data. If None, returns all normalized data.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If data has not been normalized yet.\n",
    "            IndexError: If the index is out of range.\n",
    "        \"\"\"\n",
    "        if self.__normalized_data is None:\n",
    "            raise RuntimeError(\"Data has not been normalized yet. Please call 'fit' first.\")\n",
    "        \n",
    "        if idx is not None:\n",
    "            if idx < 0 or idx >= len(self.__normalized_data):\n",
    "                raise IndexError(\"Index out of range.\")\n",
    "            return self.__normalized_data[idx]\n",
    "        \n",
    "        return self.__normalized_data  # Return all normalized data if idx is None\n",
    "\n",
    "    def get_original_data(self):\n",
    "        \"\"\"\n",
    "        Returns the original, un-normalized data.\n",
    "        \"\"\"\n",
    "        return [data.copy() for data in self.__original_data]  # Return a deep copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PEAKS Detection \n",
    "\n",
    "This function detect peaks with a delta.\n",
    "\n",
    "Choosing a default delta value for the peakdet function depends on the characteristics of your data and the desired level of peak/valley sensitivity. Here are some considerations:\n",
    "\n",
    "- Data Scale: If your data values are on a large scale (e.g., stock prices in the thousands), a larger delta might be appropriate to avoid detecting insignificant fluctuations.\n",
    "- Noise Level: If your data has a high level of noise, a larger delta might be necessary to filter out minor variations and focus on more prominent peaks and valleys.\n",
    "- Desired Sensitivity: If you want to capture a broad range of peaks and valleys, a smaller delta would be suitable. However, this might also lead to detecting insignificant pumps or dips.\n",
    "  \n",
    "Here are some possible default values based on common scenarios:\n",
    "\n",
    "- General Case: A reasonable starting point for many applications could be a delta value between 0.01 and 0.1. This range is a relative percentage of the data scale and can capture significant peaks and valleys without being overly sensitive to noise.\n",
    "- Highly Scaled Data (e.g., Stock Prices): You might consider a delta between 1.0 and 10.0 for data with large values.\n",
    "- Noisy Data: A delta between 0.05 and 0.2 could be a starting point for data with significant noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def peaks_detection(data: list[float], delta: float = 0.01, x: list[float] = None) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Finds peaks and valleys in a data series.\n",
    "\n",
    "    Args:\n",
    "        data (list[float]): The data series.\n",
    "        delta (float): The threshold for a peak or valley.\n",
    "        x (list[float], optional): The x-axis values (optional). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Two numpy arrays, the first containing the indices and values of the peaks,\n",
    "               the second containing the indices and values of the valleys.\n",
    "    \"\"\"\n",
    "\n",
    "    data_array = np.asarray(data)  # Ensure NumPy array for efficiency\n",
    "\n",
    "    if x is None:\n",
    "        x = np.arange(len(data_array))  # Create x-axis if not provided\n",
    "\n",
    "    peaks: list[tuple[float, float]] = []\n",
    "    valleys: list[tuple[float, float]] = []\n",
    "    current_peak = np.inf\n",
    "    current_valley = -np.inf\n",
    "    peak_pos = np.nan\n",
    "    valley_pos = np.nan\n",
    "    looking_for_peak = True  # Flag to track search direction\n",
    "\n",
    "    for i, this_value in enumerate(data_array):\n",
    "        # Update current peak and valley values\n",
    "        current_peak = max(current_peak, this_value)\n",
    "        current_valley = min(current_valley, this_value)\n",
    "\n",
    "        if looking_for_peak:\n",
    "            if this_value < current_peak - delta:\n",
    "                if not np.isnan(peak_pos):\n",
    "                    peaks.append((x[int(peak_pos)], current_peak))\n",
    "                # peaks.append((x[int(peak_pos)], current_peak))\n",
    "                current_valley = this_value\n",
    "                valley_pos = i\n",
    "                looking_for_peak = False\n",
    "        else:\n",
    "            if this_value > current_valley + delta:\n",
    "                if not np.isnan(valley_pos):\n",
    "                    valleys.append((x[int(valley_pos)], current_valley))\n",
    "                # valleys.append((x[int(valley_pos)], current_valley))\n",
    "                current_peak = this_value\n",
    "                peak_pos = i\n",
    "                looking_for_peak = True\n",
    "\n",
    "    return np.array(peaks), np.array(valleys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Frame labelization \n",
    "\n",
    "Currently: buy, sell, wait/neutral\n",
    "\n",
    "update to: strong buy, buy, strong sell, sell, wait/neutral (can also use the prediction probability to determine this \n",
    "e.g when a buy is >=75%, it can indicate a strong buy)\n",
    "\n",
    "can also rename wait to neutral \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketLabeler:\n",
    "    \"\"\"\n",
    "    A class for labeling market data with buy, sell, or neutral signals based on peak information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, peaks_max, peaks_min):\n",
    "        \"\"\"\n",
    "        Initializes the MarketLabeler with peak data.\n",
    "\n",
    "        Args:\n",
    "            peaks_max (list): A list of tuples representing peak maxima (index, value).\n",
    "            peaks_min (list): A list of tuples representing peak minima (index, value).\n",
    "        \"\"\"\n",
    "        self.peaks_max = peaks_max\n",
    "        self.peaks_min = peaks_min\n",
    "\n",
    "    def label_data(self, data):\n",
    "        \"\"\"\n",
    "        Labels each data point with a buy, sell, or neutral signal based on peaks.\n",
    "\n",
    "        Args:\n",
    "            data (list): A list representing the market data (e.g., prices).\n",
    "\n",
    "        Returns:\n",
    "            list: A list of labels (0 for neutral, 1 for sell, 2 for buy) corresponding to each data point.\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        for i, _ in enumerate(data):  # Iterate through data indices\n",
    "            if any(idx == i for idx, _ in self.peaks_min):\n",
    "                labels.append(0)  # Buy signal if peak min\n",
    "            elif any(idx == i for idx, _ in self.peaks_max):\n",
    "                labels.append(1)  # Sell signal if peak max\n",
    "            else:\n",
    "                labels.append(2)  # Neutral otherwise\n",
    "        return labels\n",
    "\n",
    "    def label_dataframe(self, frame_base: pd.DataFrame, data_column: str = \"close\"):\n",
    "        \"\"\"\n",
    "        Labels a DataFrame with buy, sell, and neutral columns based on peak information.\n",
    "\n",
    "        Args:\n",
    "            frame_base (pd.DataFrame): The DataFrame containing the market data.\n",
    "            data_column (str, optional): The name of the column containing the data to be labeled. Defaults to \"close\".\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with added \"buy\", \"sell\", and \"neutral\" columns.\n",
    "        \"\"\"\n",
    "        if data_column not in frame_base.columns:\n",
    "            raise ValueError(f\"Data column '{data_column}' not found in the DataFrame.\")\n",
    "\n",
    "        labels = self.label_data(frame_base[data_column])\n",
    "        \n",
    "        frame_base[\"buy\"] = [1 if label == 0 else 0 for label in labels]\n",
    "        frame_base[\"sell\"] = [1 if label == 1 else 0 for label in labels]\n",
    "        frame_base[\"neutral\"] = [1 if label == 2 else 0 for label in labels]\n",
    "    \n",
    "        return frame_base\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class that generates data for training and testing.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        timestep (int): The number of timesteps to consider for each sample.\n",
    "        xcols (List[str]): The column names to be used as input features.\n",
    "        ycols (List[str]): The column names to be used as output labels.\n",
    "\n",
    "    Methods:\n",
    "        generate_data: Generates the input-output pairs for training and testing.\n",
    "        balance_labelization: Balances the label distribution by removing excess neutral labels.\n",
    "        train_test_split: Splits the data into training and testing sets.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: pd.DataFrame, timestep: int, xcols: List[str], ycols: List[str]):\n",
    "        self.dataset = dataset\n",
    "        self.timestep = timestep\n",
    "        self.xcols = xcols\n",
    "        self.ycols = ycols\n",
    "\n",
    "    def generate_data(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generates the input-output pairs for training and testing.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: A tuple containing the input and output arrays.\n",
    "\n",
    "        \"\"\"\n",
    "        dx = [np.array(self.dataset.iloc[i : i + self.timestep][self.xcols]) for i in range(len(self.dataset) - self.timestep)]\n",
    "        dy = [self.dataset.iloc[i + self.timestep - 1][self.ycols] for i in range(len(self.dataset) - self.timestep)]\n",
    "        return np.array(dx), np.array(dy)\n",
    "\n",
    "    def balance_labelization(self, frame: np.ndarray, label: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Balances the label distribution by removing excess neutral labels.\n",
    "\n",
    "        Args:\n",
    "            frame (np.ndarray): The input array.\n",
    "            label (np.ndarray): The label array.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: A tuple containing the balanced input and label arrays.\n",
    "\n",
    "        \"\"\"\n",
    "        neutral_indices = np.where(label[:, 2] == 1)[0]\n",
    "        sell_indices = np.where(label[:, 1] == 1)[0]\n",
    "        buy_indices = np.where(label[:, 0] == 1)[0]\n",
    "\n",
    "        neutral_count = len(neutral_indices)\n",
    "        sell_count = len(sell_indices)\n",
    "        buy_count = len(buy_indices)\n",
    "\n",
    "        need_delete = neutral_count - min(sell_count, buy_count)\n",
    "        rand_delete = np.random.choice(neutral_indices, need_delete, replace=False)\n",
    "\n",
    "        final_frame = np.delete(frame, rand_delete, axis=0)\n",
    "        final_label = np.delete(label, rand_delete, axis=0)\n",
    "\n",
    "        return final_frame, final_label\n",
    "\n",
    "    def train_test_split(self, test_per: float, balance: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Splits the data into training and testing sets.\n",
    "\n",
    "        Args:\n",
    "            test_per (float): The percentage of data to be used for testing.\n",
    "            balance (bool, optional): Whether to balance the label distribution. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: A tuple containing the training and testing input and label arrays.\n",
    "\n",
    "        \"\"\"\n",
    "        x_tmp, y_tmp = self.generate_data()\n",
    "\n",
    "        if balance:\n",
    "            x_tmp, y_tmp = self.balance_labelization(x_tmp, y_tmp)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_tmp, y_tmp, test_size=test_per, random_state = 100, shuffle = settings['model_shuffle'])\n",
    "\n",
    "        return x_train.astype(np.float32), y_train.astype(np.float32), x_test.astype(np.float32), y_test.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Add available technical analysis (TA) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_ta_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Adds various technical analysis (TA) features to the provided DataFrame.\n",
    "    \n",
    "    This function calculates several technical indicators, including momentum, \n",
    "    volatility, statistics, trend, volume, overlap, and performance indicators. \n",
    "    The input DataFrame must include a \"date\" column for setting the index to \n",
    "    a DatetimeIndex.\n",
    "\n",
    "    References:\n",
    "    - https://github.com/twopirllc/pandas-ta/blob/main/examples/AIExample.ipynb\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    features_df = df.copy() \n",
    "\n",
    "    # VWAP requires the DataFrame index to be a DatetimeIndex.\n",
    "    # Replace \"datetime\" with the appropriate column from your DataFrame\n",
    "    \n",
    "    features_df.set_index(pd.DatetimeIndex(features_df.index), inplace=True)\n",
    "    try:\n",
    "        features_df.drop(columns=[features_df.index.name], inplace=True)\n",
    "    except KeyError:\n",
    "        print(f\"Column '{features_df.index.name}' does not exist in the DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while dropping the column: {e}\")\n",
    "\n",
    "\n",
    "    # Calculate Returns and append to the features_df DataFrame\n",
    "    # features_df.ta.log_return(cumulative=True, append=True)\n",
    "    # features_df.ta.percent_return(cumulative=True, append=True)\n",
    "\n",
    "    \"\"\" \n",
    "        Add TA Indicators\n",
    "    \"\"\"\n",
    "    \n",
    "    # Adding momentum indicators\n",
    "    features_df.ta.mom(append=True)\n",
    "    features_df.ta.rsi(append=True)\n",
    "    features_df.ta.tsi(append=True)\n",
    "    features_df.ta.er(append=True)\n",
    "    features_df.ta.fisher(append=True)\n",
    "\n",
    "    # Adding volatility indicators\n",
    "    features_df.ta.true_range(append=True)\n",
    "    features_df.ta.rvi(append=True)\n",
    "    features_df.ta.bbands(append=True)\n",
    "    features_df.ta.pdist(append=True)\n",
    "\n",
    "    # Adding statistics indicators\n",
    "    features_df.ta.skew(append=True)\n",
    "    features_df.ta.kurtosis(append=True)\n",
    "    features_df.ta.mad(append=True)\n",
    "    features_df.ta.zscore(append=True)\n",
    "    features_df.ta.entropy(append=True)\n",
    "\n",
    "    # Adding trend indicators\n",
    "    features_df.ta.adx(append=True)\n",
    "    features_df.ta.dpo(lookahead=False, append=True)\n",
    "    features_df.ta.psar(append=True)\n",
    "    features_df.ta.long_run(append=True)\n",
    "    features_df.ta.short_run(append=True)\n",
    "    features_df.ta.qstick(append=True)\n",
    "\n",
    "    # Adding volume indicators\n",
    "    # features_df.ta.obv(append=True)\n",
    "\n",
    "    # Adding overlap indicators\n",
    "    features_df.ta.linreg(append=True)\n",
    "    features_df.ta.supertrend(append=True)\n",
    "    features_df.ta.hilo(append=True)\n",
    "    features_df.ta.hlc3(append=True)\n",
    "    features_df.ta.ohlc4(append=True)\n",
    "\n",
    "    # Adding Simple Moving Averages (SMA)\n",
    "    sma_windows = [2, 10, 15, 60]\n",
    "    for ma_window in sma_windows:\n",
    "        features_df.ta.sma(length=ma_window, sma=False, append=True)\n",
    "\n",
    "    # Adding Exponential Moving Averages (EMA)\n",
    "    ema_windows = [8, 21, 50]\n",
    "    for ma_window in ema_windows:\n",
    "        features_df.ta.ema(length=ma_window, sma=False, append=True)\n",
    "\n",
    "    # Adding performance indicators\n",
    "    # features_df.ta.percent_return(append=True)\n",
    "\n",
    "    print(\"TA Columns Added\")\n",
    "\n",
    "    \"\"\" \n",
    "    #### Modify RSI (Relative Strength Index) Indicator\n",
    "\n",
    "        - The RSI provides technical traders with signals about bullish and bearish price momentum, \n",
    "        and it is often plotted beneath the graph of an assetâ€™s price.\n",
    "\n",
    "        - An asset is usually considered overbought when the RSI is above 70 and oversold when it is below 30.\n",
    "\n",
    "        - The RSI line crossing below the overbought line or above the oversold line is often seen by traders \n",
    "        as a signal to buy or sell.\n",
    "\n",
    "        - The RSI works best in trading ranges rather than trending markets.\n",
    "    \"\"\"\n",
    "    features_df[\"RSI_14\"] = features_df[\"RSI_14\"].round()  # Rounding RSI values to the nearest integer\n",
    "\n",
    "    # Add Candle Stick Patterns   \n",
    "    features_df.ta.cdl_pattern(name=\"all\", append=True)\n",
    "\n",
    "    return features_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define features engineering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "\n",
    "def features_engineering(df: pd.DataFrame, add_ta: bool = False, compress_features: bool = True) -> dict:\n",
    "    \"\"\" \n",
    "    This function creates the necessary datasets for algorithms \n",
    "    by performing feature engineering, scaling, and PCA.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe containing price data (including 'close', 'high', 'low').\n",
    "        add_ta (bool): Whether to add technical analysis features. Defaults to False.\n",
    "        compress_features (bool): Whether to apply PCA for dimensionality reduction. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the train, test, validation sets, \n",
    "              along with their scaled and PCA-transformed versions.\n",
    "    \"\"\"\n",
    "\n",
    "    original_df = df.copy()\n",
    "\n",
    "    # Define the feature columns\n",
    "    feature_columns = [\n",
    "        \"returns t-1\", \n",
    "        \"mean returns 15\", \n",
    "        \"mean returns 60\",\n",
    "        \"volatility returns 15\", \n",
    "        \"volatility returns 60\"\n",
    "    ]\n",
    "    \n",
    "    # Create new columns for returns and other features\n",
    "\n",
    "    # Ensure returns column is in the dataframe\n",
    "    df[\"returns\"] = (df[\"close\"] - df[\"close\"].shift(1)) / df[\"close\"].shift(1)\n",
    "    df[\"sLow\"] = (df[\"low\"] - df[\"close\"].shift(1)) / df[\"close\"].shift(1)\n",
    "    df[\"sHigh\"] = (df[\"high\"] - df[\"close\"].shift(1)) / df[\"close\"].shift(1)\n",
    "\n",
    "    # Feature engineering\n",
    "    df[\"returns t-1\"] = df[\"returns\"].shift(1)\n",
    "    df[\"mean returns 15\"] = df[\"returns\"].rolling(15).mean().shift(1)\n",
    "    df[\"mean returns 60\"] = df[\"returns\"].rolling(60).mean().shift(1)\n",
    "    df[\"volatility returns 15\"] = df[\"returns\"].rolling(15).std().shift(1)\n",
    "    df[\"volatility returns 60\"] = df[\"returns\"].rolling(60).std().shift(1)\n",
    "\n",
    "    # Add technical analysis features if specified\n",
    "    if add_ta:\n",
    "        df = add_ta_features(df)\n",
    "        feature_columns = df.columns.to_list()\n",
    "\n",
    "    # Impute missing values using IterativeImputer (Multiple Imputation by Chained Equations)\n",
    "    # imputer = IterativeImputer(random_state=0)\n",
    "    # df[feature_columns + [\"returns\"]] = imputer.fit_transform(df[feature_columns + [\"returns\"]])\n",
    "    # df[feature_columns + [\"returns\"]] = df[feature_columns + [\"returns\"]].fillna(0)\n",
    "    # print(df)\n",
    "\n",
    "    # # Splitting data into train, test, and validation sets\n",
    "    # split_train_test = int(0.70 * len(df))\n",
    "    # split_test_valid = int(0.90 * len(df))\n",
    "\n",
    "    # # Train set creation\n",
    "    # X_train = df[feature_columns].iloc[:split_train_test]\n",
    "    # y_train_reg = df[\"returns\"].iloc[:split_train_test]\n",
    "    # y_train_cla = np.round(df[\"returns\"].iloc[:split_train_test] + 0.5)\n",
    "\n",
    "    # # Test set creation\n",
    "    # X_test = df[feature_columns].iloc[split_train_test:split_test_valid]\n",
    "    # y_test_reg = df[\"returns\"].iloc[split_train_test:split_test_valid]\n",
    "\n",
    "    # # Validation set creation\n",
    "    # X_val = df[feature_columns].iloc[split_test_valid:]\n",
    "    # y_val_reg = df[\"returns\"].iloc[split_test_valid:]\n",
    "\n",
    "    # # Standardize the data\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train_scaled = scaler.fit_transform(X_train)\n",
    "    # X_test_scaled = scaler.transform(X_test)\n",
    "    # X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Use the Normalizer for scaling\n",
    "    # normalizer = Normalizer(data=[{\"name\": \"X_train\", \"data\": X_train}])\n",
    "    # normalizer.fit(\"StandardScale\", feature_columns)\n",
    "    # # norm.fit(settings[\"normalization_fit_type\"], settings[\"features_name\"], settings[\"normalization_fit_target\"])]\n",
    "\n",
    "    # # Get normalized data\n",
    "    # X_train_scaled = normalizer.get_normalized_data(0)  # Get the scaled data\n",
    "    # X_test_scaled = normalizer.get_normalized_data(1)  # Get scaled test data\n",
    "    # X_val_scaled = normalizer.get_normalized_data(2)  # Get scaled validation data\n",
    "\n",
    "    # Prepare the features dictionary\n",
    "    features_dict = {\n",
    "        \"df\": original_df,\n",
    "        \"processed_df\": df,\n",
    "        \"feature_columns\": feature_columns, \n",
    "        # \"X_train\": X_train, \n",
    "        # \"X_test\": X_test, \n",
    "        # \"y_train_reg\": y_train_reg, \n",
    "        # \"y_train_cla\": y_train_cla,\n",
    "        # \"X_train_scaled\": X_train_scaled,\n",
    "        # \"split_train_test\": split_train_test,\n",
    "        # \"split_test_valid\": split_test_valid,\n",
    "        # \"X_test_scaled\": X_test_scaled,\n",
    "        # \"y_test_reg\": y_test_reg,\n",
    "        # \"X_val\": X_val,\n",
    "        # \"X_val_scaled\": X_val_scaled,\n",
    "        # \"y_val_reg\": y_val_reg,\n",
    "    }\n",
    "\n",
    "    # # Apply PCA if specified\n",
    "    # if compress_features:\n",
    "    #     pca = PCA(n_components=3)\n",
    "    #     X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    #     X_test_pca = pca.transform(X_test_scaled)\n",
    "    #     X_val_pca = pca.transform(X_val_scaled)\n",
    "\n",
    "    #     features_dict[\"X_train_pca\"] = X_train_pca\n",
    "    #     features_dict[\"X_test_pca\"] = X_test_pca\n",
    "    #     features_dict[\"X_val_pca\"] = X_val_pca\n",
    "\n",
    "    # Return all relevant datasets as a dictionary for clarity and organization\n",
    "    return features_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Deep Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_assets[\"Symbol\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "The \"horizon\" in the settings refers to how many steps into the future the model is tasked with predicting, using the current and past data points. In your case:\n",
    "\n",
    "- **Horizon (int)**: This specifies the number of time steps or terms that the model should predict into the future.\n",
    "  \n",
    "  For example, if you set the horizon to `120`, this means the model will attempt to predict the next 120 future time steps based on the historical data it is trained on. \n",
    "\n",
    "- **Used Time Frame**: In this case, we are using a 1-minute time frame, which means that each time step represents 1 minute of data.\n",
    "\n",
    "- **Total Future Time Covered**: The horizon multiplied by the time frame gives you the total future time that the model is expected to see or predict.\n",
    "  \n",
    "  - For example, with a 1-minute time frame and a horizon of 120, the model is expected to predict the next 120 minutes (i.e., 2 hours) into the future.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "**Total future time = horizon X used time frame**\n",
    "\n",
    "In this case, using the 15-minute time frame:\n",
    "- With a **8 horizon**, the total prediction span will be:\n",
    "  **8 X 15 min = 120 minutes (2 hours)**\n",
    "\n",
    "Thus, the model is expected to predict up to 2 hours into the future using a 120-minute horizon with 15-minute granularity.\n",
    "\n",
    "Mins in a day = 24 * 60 * 60 = 93600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "                 \"target_markets\": ['Volatility 25 Index', 'Volatility 75 (1s) Index', 'Boom 500 Index', 'Step Index'],\n",
    "                 \"normalization_fit_type\": \"StandardScale\",\n",
    "                 \"normalization_fit_target\": \"all\",\n",
    "                 \"normalization_target\": \"close\",\n",
    "                 \"label_disparity\": 0.095, # 0.025, 0.045, 0.055, 0.065, 0.075, 0.085, 0.095, 0.1\n",
    "                 \"features_name\": ['open', 'high', 'low', 'close', 'volume'], # [\"ask\", \"bid\", \"high\", \"low\", \"moy_prev_day\"], \n",
    "                 \"labels_name\": [\"buy\", \"sell\", \"neutral\"], # [\"buy\", \"sell\", \"neutral\"] | [\"buy\", \"sell\", \"wait\"] | [\"strong buy\", \"buy\", \"strong sell\", \"sell\", \"neutral\"]\n",
    "                 \"is_balance\": True,\n",
    "                 \"split_train\": 0.8,\n",
    "                 \"split_test\": 0.2,\n",
    "                 \"horizon\": 8, \n",
    "                 \"delta_neurons_numbers\": 4, # delta {2-10}\n",
    "                 \"model_optimizer\": \"adam\",\n",
    "                 \"model_loss\": \"CrossEntropyLoss\", \n",
    "                 \"model_epoch\": 120, # 50, 70, 100, 120, 150, 250\n",
    "                 \"model_batch_size\": 32, # 50, 32, 25\n",
    "                 \"model_validation_split\": 0.15,\n",
    "                 \"model_shuffle\": False\n",
    "                }\n",
    "\n",
    "TIME_FRAME = mt5.TIMEFRAME_M15\n",
    "LABEL_TARGET: str = settings[\"normalization_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# Initialize MetaTrader 5 or other data source\n",
    "mt5.initialize()\n",
    "\n",
    "# Symbols list from final_assets\n",
    "dl_selected_symbols = settings['target_markets']\n",
    "\n",
    "dl_selected_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conversion factors\n",
    "# years_to_days = 365.25  # Average days in a year accounting for leap years\n",
    "# days_to_hours = 24  # Hours in a day\n",
    "# hours_to_minutes = 60  # Minutes in an hour\n",
    "\n",
    "# # Converting years to minutes\n",
    "# years = 3\n",
    "# minutes = years * years_to_days * days_to_hours * hours_to_minutes\n",
    "# minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Data with All Available TA Indicators \n",
    "\n",
    "From January 2022 to January 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store results\n",
    "processed_data_results = []\n",
    "\n",
    "def process_symbol(symbol):\n",
    "    try:\n",
    "        print(f\"Processing symbol: {symbol}\")\n",
    "        # Retrieve data for the symbol\n",
    "        df = get_data(symbol, 0, timeframe=TIME_FRAME, is_range=True, \n",
    "                      utc_from=datetime(2022, 1, 1), utc_to=datetime(2024, 1, 1)).dropna()\n",
    "        # print(df)\n",
    "        \n",
    "        # Perform feature engineering\n",
    "        processed_data = features_engineering(df, add_ta=True)\n",
    "        settings['features_name'] = processed_data['feature_columns']\n",
    "        return processed_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Issue during data importation or processing for symbol {symbol}: {e}\")\n",
    "        return None  # Return None in case of an error\n",
    "\n",
    "# Use ThreadPoolExecutor or ProcessPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Use tqdm to show progress\n",
    "    futures = {executor.submit(process_symbol, symbol): symbol for symbol in dl_selected_symbols}\n",
    "\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            processed_data_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_data_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_results[0]['processed_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display\n",
    "for i, name in enumerate(dl_selected_symbols):\n",
    "    fig = plt.figure(figsize=(21, 7))\n",
    "\n",
    "    # Plot the target label's data\n",
    "    processed_data_results[i]['processed_df'][LABEL_TARGET].plot(label=LABEL_TARGET, title=f\"{LABEL_TARGET} Price Curve\".capitalize())\n",
    "\n",
    "    # Add a watermark or name to the background\n",
    "    plt.text(\n",
    "    0.5, 0.5,                    # X and Y position (centered in the plot)\n",
    "    name,                          # The text to display\n",
    "    fontsize=50,                  # Font size for visibility\n",
    "    color='gray',                 # Color of the watermark\n",
    "    alpha=0.3,                    # Transparency level\n",
    "    ha='center',                  # Horizontal alignment\n",
    "    va='center',                  # Vertical alignment\n",
    "    transform=plt.gca().transAxes  # Ensure the text is relative to the plot's axes\n",
    "    )\n",
    "\n",
    "    # Display grid and legend\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Nomalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings[\"features_name\"] = processed_data_results[0]['processed_df'].columns\n",
    "\n",
    "settings[\"features_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = []\n",
    "for i, name in enumerate(dl_selected_symbols):\n",
    "    ndata.append({\"name\": name, \"data\": processed_data_results[i]['processed_df']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION OF THE DATA\n",
    "norm = Normalizer(data=ndata)\n",
    "norm.fit(settings[\"normalization_fit_type\"], settings[\"features_name\"], settings[\"normalization_fit_target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data_list = []\n",
    "for i in range(len(dl_selected_symbols)):\n",
    "    normalize_data_list.append(norm.get_normalized_data(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PEAKS DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_peak_distance(peaks_max, peaks_min):\n",
    "    \"\"\"\n",
    "    Estimate peak distance between maximum and minimum peaks.\n",
    "\n",
    "    Args:\n",
    "        peaks_max (np.ndarray): Array of maximum peak values.\n",
    "        peaks_min (np.ndarray): Array of minimum peak values.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the mean and distance values for both peaks_max and peaks_min.\n",
    "    \"\"\"\n",
    "    # Flatten the peak arrays\n",
    "    peaks_max_flat = peaks_max.flatten()\n",
    "    peaks_min_flat = peaks_min.flatten()\n",
    "\n",
    "    # Calculate mean values\n",
    "    mean_max = np.mean(peaks_max_flat)\n",
    "    mean_min = np.mean(peaks_min_flat)\n",
    "\n",
    "    # Calculate the difference between max peak and mean\n",
    "    distance_max = np.max(peaks_max_flat) - mean_max\n",
    "    distance_min = np.max(peaks_min_flat) - mean_min\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        'mean_max': mean_max,\n",
    "        'mean_min': mean_min,\n",
    "        'distance_max': distance_max,\n",
    "        'distance_min': distance_min\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_peaks_for_all(normalize_data_list, settings):\n",
    "    \"\"\"\n",
    "    Dynamically detect peaks (max and min) for each dataset in the list.\n",
    "\n",
    "    Args:\n",
    "        normalize_data_list (list): List of normalized data arrays.\n",
    "        settings (dict): Settings containing normalization target and label disparity.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing (peaks_max, peaks_min) for each dataset.\n",
    "    \"\"\"\n",
    "    peaks_results = []\n",
    "\n",
    "    for i, normalize_data in enumerate(normalize_data_list):\n",
    "        # Apply peaks detection for each dataset in the list\n",
    "        peaks_max, peaks_min = peaks_detection(normalize_data[settings[\"normalization_target\"]], settings[\"label_disparity\"])\n",
    "        \n",
    "        # Append the results as a tuple (peaks_max, peaks_min)\n",
    "        peaks_results.append((peaks_max, peaks_min))\n",
    "\n",
    "    return peaks_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peaks_max, peaks_min = peaks_detection(normalize_data[settings[\"normalization_target\"]], settings[\"label_disparity\"])\n",
    "peaks_results = detect_peaks_for_all(normalize_data_list, settings)\n",
    "\n",
    "peaks_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, peaks_result in enumerate(peaks_results):\n",
    "    peak_max, peak_min = peaks_results\n",
    "    print(f\"{i} => \", estimate_peak_distance(peak_max, peak_min))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display peaks\n",
    "fig = plt.figure(figsize=(21, 7))\n",
    "plt.plot(peaks_max[:,0], peaks_max[:, 1], 'ro', label=\"Maximum peaks\")\n",
    "plt.plot(peaks_min[:,0], peaks_min[:, 1], 'go', label=\"Minimum valleys\")\n",
    "plt.plot(normalize_data[LABEL_TARGET], label=LABEL_TARGET.capitalize())\n",
    "plt.grid()\n",
    "plt.title(\"Peaks detection\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Frame labelization\n",
    "\n",
    "LABELIZATION WITH PEAKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "labeler = MarketLabeler(peaks_max, peaks_min)\n",
    "labeled_normalize_df = labeler.label_dataframe(normalize_data, LABEL_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_normalize_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Data Generator - Split Normalized Data\n",
    "\n",
    "horizon (int): The number of forward terms of the target time series to be estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(labeled_normalize_df, settings[\"horizon\"], settings[\"features_name\"], settings[\"labels_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_train, y_train, x_test, y_test = data_generator.train_test_split(settings[\"split_test\"], settings[\"is_balance\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.dtype, y_train.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Dense, Dropout, Bidirectional, Reshape\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.metrics import F1Score, FBetaScore, Precision, Recall \n",
    "# from tcn import TCN, tcn_full_summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from pytorch_tcn import TCN\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# https://github.com/paul-krug/pytorch-tcn\n",
    "# https://github.com/locuslab/TCN/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model Util functions (for Neural Nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lstm_neurons(data_array: np.array, settings: dict) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the number of neurons for an LSTM network based on data and settings.\n",
    "\n",
    "    Args:\n",
    "        data_array (np.array): The numpy array containing the training data.\n",
    "        settings (dict): A dictionary containing settings for neuron calculation.\n",
    "            - features_name (list): Names of features for training.\n",
    "            - labels_name (list): Names of output labels (classes).\n",
    "            - delta_neurons_numbers (int): Hyperparameter scaling the number of neurons.\n",
    "\n",
    "    Returns:\n",
    "        int: The calculated number of neurons for the LSTM network.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required keys are missing from the settings dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    if not all(key in settings for key in [\"features_name\", \"labels_name\", \"delta_neurons_numbers\"]):\n",
    "        raise ValueError(\"Missing required keys in 'settings' dictionary.\")\n",
    "\n",
    "    num_features = len(settings[\"features_name\"])\n",
    "    num_labels = len(settings[\"labels_name\"])\n",
    "    num_samples = data_array.shape[0]\n",
    "    scaling_factor = settings[\"delta_neurons_numbers\"]\n",
    "\n",
    "    return int(num_samples / (scaling_factor * (num_features + num_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_train_test_set(x_train, x_test, y_train, y_test):\n",
    "    \"\"\"Creates train and test datasets from numpy arrays or lists.\n",
    "\n",
    "    Args:\n",
    "        x_train: Training features.\n",
    "        x_test: Test features.\n",
    "        y_train: Training labels.\n",
    "        y_test: Test labels.\n",
    "\n",
    "    Returns:\n",
    "        training_data: A TensorDataset for the training set.\n",
    "        test_data: A TensorDataset for the test set.\n",
    "    \"\"\"\n",
    "    # Convert to torch tensors\n",
    "    tensor_x_train = torch.Tensor(x_train)\n",
    "    tensor_y_train = torch.Tensor(y_train)\n",
    "    training_data = TensorDataset(tensor_x_train, tensor_y_train)  # Create training dataset\n",
    "\n",
    "    tensor_x_test = torch.Tensor(x_test)\n",
    "    tensor_y_test = torch.Tensor(y_test)\n",
    "    test_data = TensorDataset(tensor_x_test, tensor_y_test)  # Create test dataset\n",
    "\n",
    "    return training_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm\n",
    "# x_tcn = GlobalAveragePooling1D()(x_tcn)\n",
    " \n",
    "# SET NUMBER OF NEURONS\n",
    "settings[\"model_neural\"] = calculate_lstm_neurons(x_train, settings)  \n",
    "settings[\"model_neural_hidden\"] =  settings[\"model_neural\"] + int(settings[\"model_neural\"] / 2) # int(settings[\"model_neural\"] / 2)\n",
    "\n",
    "settings[\"model_neural\"], settings[\"model_neural_hidden\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = x_train.shape[2]  # Feature size per time step (input dimension)\n",
    "hidden_dim_1 = settings[\"model_neural\"]  # Hidden layer size\n",
    "hidden_dim_2 = settings[\"model_neural_hidden\"]  # Hidden layer size\n",
    "output_dim = y_train.shape[1]  # Number of output classes\n",
    "device = \"cpu\"\n",
    "models = {}\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = create_train_test_set(x_train, x_test, y_train, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=settings[\"model_batch_size\"], shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=settings[\"model_batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model A (Bi-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers: int = 4, **kwargs):\n",
    "        super(BidirectionalLSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size or 24\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Bidirectional LSTMs\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "\n",
    "        # Dropout layer\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Dense (fully connected) layer with softmax activation\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        # h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        # c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Passing input through LSTMs\n",
    "        out, _ = self.bilstm(x) # out, hidden = self.bilstm(x, (h0, c0))\n",
    "        # out = self.dropout(out)\n",
    "\n",
    "        # Taking the output of the last time step\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        fc = self.fc(out)\n",
    "        x = F.softmax(fc)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model instantiation\n",
    "models[\"BidirectionalLSTM\"] = BidirectionalLSTMModel(input_size=input_dim, hidden_size=hidden_dim_1, output_size=output_dim)\n",
    "\n",
    "# Summary \n",
    "print(models[\"BidirectionalLSTM\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model B (Bi-GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers: int = 4, **kwargs):\n",
    "        super(BidirectionalGRUModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size or 24\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Bidirectional GRU layers\n",
    "        self.bigru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        # Dense (fully connected) layer with softmax activation\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through bidirectional GRU layers\n",
    "        out, _ = self.bigru(x)\n",
    "        \n",
    "        # Taking the output of the last time step\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        fc = self.fc(out)\n",
    "        x = F.softmax(fc)\n",
    "        return x\n",
    "\n",
    "models[\"BidirectionalGRU\"] = BidirectionalGRUModel(input_size=input_dim, hidden_size=hidden_dim_1, output_size=output_dim)\n",
    "\n",
    "# Summary \n",
    "print(models[\"BidirectionalGRU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model C (TCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels: Tuple = (2, 3, 5, 8, 16, 32), kernel_size: int = 5, dilations: Tuple = (1, 2, 4, 8, 16, 32)):\n",
    "        super(TCNModel, self).__init__()\n",
    "\n",
    "        # Define the TCN layer with appropriate parameters\n",
    "        self.tcn = TCN(input_size, num_channels, kernel_size, dilations, input_shape='NLC', dropout=0.2)\n",
    "\n",
    "        # Fully connected layer with input size matching the last TCN channel size\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passing input through the TCN layer\n",
    "        out = self.tcn(x)\n",
    "\n",
    "        # Taking the output of the last time step\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        fc = self.fc(out)\n",
    "        \n",
    "        x = F.softmax(fc)\n",
    "        return x  \n",
    "\n",
    "\n",
    "# Model instantiation\n",
    "models[\"TCN\"] = TCNModel(input_size=input_dim, output_size=output_dim)\n",
    "\n",
    "# Summary \n",
    "print(models[\"TCN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module,\n",
    "                model_optimizer: str,\n",
    "                model_loss: str,\n",
    "                num_epochs: int = 20,\n",
    "                dataloader: Optional[DataLoader] = None,\n",
    "                device: Optional[str] = None) -> tuple[nn.Module, optim.Optimizer, nn.Module]:\n",
    "    \"\"\"Trains a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to train.\n",
    "        model_optimizer: The name of the optimizer to use.\n",
    "        model_loss: The name of the loss function to use.\n",
    "        num_epochs: The number of epochs to train for.\n",
    "        dataloader: The DataLoader object for loading data.\n",
    "        device: The device to use for training (e.g., 'cpu', 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the trained model, optimizer, and loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataloader is None:\n",
    "        raise ValueError(\"A DataLoader object must be provided.\")\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer_class = getattr(optim, model_optimizer.capitalize())\n",
    "    optimizer = optimizer_class(model.parameters())\n",
    "\n",
    "    # Loss function\n",
    "    loss_function = getattr(nn, model_loss)()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model, optimizer, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items(): \n",
    "    print(f\"{name} Model:\")\n",
    "    trained_models[name] = train_model(models[name], settings[\"model_optimizer\"], settings[\"model_loss\"], settings['model_epoch'], train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot real-time\n",
    "# plotRT = PlotRealTime()\n",
    "\n",
    "# # Learning stats\n",
    "# def lr(epoch):\n",
    "#     if epoch < 15:\n",
    "#         return 0.0001 # 0.010\n",
    "#     if epoch < 40:\n",
    "#         return 0.0095\n",
    "#     if epoch < 55:\n",
    "#         return 0.008\n",
    "#     if epoch < 60:\n",
    "#         return 0.007\n",
    "#     if epoch < 150:\n",
    "#         return 0.0001 # 0.01\n",
    "    \n",
    "#     return 0.0001\n",
    "\n",
    "# def lr2(epoch):\n",
    "#     if epoch < 100:\n",
    "#         return 1e-8\n",
    "    \n",
    "#     return 1e-4\n",
    "\n",
    "\n",
    "# # Define early stopping callback\n",
    "# patience = 25\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# plotter = PlotRealTime()\n",
    "# history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[plotter])\n",
    "\n",
    "\n",
    "# %%time\n",
    "# hist_a = model_a.fit(x_train,\n",
    "#                  y_train, \n",
    "#                  epochs=settings[\"model_epoch\"],\n",
    "#                  batch_size=settings[\"model_batch_size\"],\n",
    "#                  validation_split=settings[\"model_validation_split\"],\n",
    "#                  verbose=1,\n",
    "#                  shuffle=settings[\"model_shuffle\"],\n",
    "#                  callbacks=[plotRT, early_stopping, LearningRateScheduler(lr, verbose=1)])\n",
    "\n",
    "# model_a.save(f\"../models/{file_name}_model_a_{time.time()}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "np.argmax(y_test, axis=1) => {0: 'BUY', '1': 'SELL', 2: 'NEUTRAL'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "def evaluate_model(trained_model: Tuple[nn.Module, Optimizer, nn.Module],\n",
    "                   dataloader: DataLoader,\n",
    "                   device: Optional[str] = None) -> dict:\n",
    "    \"\"\"Evaluates a PyTorch model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        trained_model: A tuple containing the trained PyTorch model, optimizer, and loss function.\n",
    "        dataloader: The DataLoader object for loading evaluation data.\n",
    "        device: The device to use for evaluation (e.g., 'cpu', 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing evaluation metrics like accuracy, precision, recall, F1 score, and confusion matrix.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model, optimizer, loss_function = trained_model\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "    raw_targets = []\n",
    "    raw_predictions = []\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    total_loss = 0\n",
    "\n",
    "    # Loss function for evaluation \n",
    "    criterion = loss_function\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            raw_targets.extend(targets.cpu().numpy())\n",
    "            raw_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "            # Convert outputs to predicted class (taking the argmax for multi-class classification)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # If targets are one-hot encoded, convert them to class indices\n",
    "            if len(targets.shape) > 1 and targets.shape[1] > 1:\n",
    "                targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "    # Return the evaluation metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / len(dataloader),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"raw_targets\": raw_targets,\n",
    "        \"raw_predictions\": raw_predictions,\n",
    "    }\n",
    "\n",
    "    # Display the metrics\n",
    "    print(f\"Evaluation Loss: {metrics['loss']:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in trained_models.items():\n",
    "    print(f\"{name} Model:\")\n",
    "    metrics[name] = evaluate_model(trained_models[name], test_dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_results = pd.DataFrame(metrics).T.drop(columns=['raw_targets', 'raw_predictions'])\n",
    "\n",
    "dl_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, label_map: dict = {0: 'BUY', 1: 'SELL', 2: 'NEUTRAL'}):\n",
    "    \"\"\"Plots the confusion matrix using a heatmap.\n",
    "\n",
    "    Args:\n",
    "        conf_matrix: The confusion matrix to plot (numpy array or similar).\n",
    "        label_map: A dictionary mapping label indices to actual class names.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    # Create heatmap with the confusion matrix\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', ax=ax)  # annot=True to annotate cells, fmt='g' for plain numbers\n",
    "\n",
    "    # Add labels, title, and ticks\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('Actual Labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    # Set tick labels based on label_map\n",
    "    labels = list(label_map.values())\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, metric in metrics.items():\n",
    "    print(f\"{name} Model:\")\n",
    "    conf_matrix = metric['confusion_matrix']\n",
    "    plot_confusion_matrix(conf_matrix, label_map={0: 'BUY', 1: 'SELL', 2: 'NEUTRAL'})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_roc_curve(y_pred, y_test, classes=[0, 1, 2]):\n",
    "    \"\"\"\n",
    "    Plots the ROC curves for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        y_pred (numpy array): Predicted probabilities for each class, shape (n_samples, n_classes).\n",
    "        y_test (numpy array): True class labels, shape (n_samples,).\n",
    "        classes (list): List of classes. Default is [0, 1, 2].\n",
    "\n",
    "    Returns:\n",
    "        None. Displays the ROC curve plot.\n",
    "    \"\"\"\n",
    "    # Binarize the output for multi-class ROC\n",
    "    y_test_bin = label_binarize(y_test, classes=classes)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Initialize dictionaries to store False Positive Rates, True Positive Rates, and AUCs for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    # Compute ROC curve and AUC for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and AUC\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and AUC\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "    # Interpolate all ROC curves and compute the mean True Positive Rate (macro-average)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot the ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Micro-average ROC curve\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'Micro-average ROC curve (area = {roc_auc[\"micro\"]:0.2f})',\n",
    "             color='blue', linestyle=':', linewidth=4)\n",
    "\n",
    "    # Macro-average ROC curve\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label=f'Macro-average ROC curve (area = {roc_auc[\"macro\"]:0.2f})',\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    # ROC curves for each class\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of class {classes[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    # Plot the diagonal (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, metric in metrics.items():\n",
    "    print(f\"{name} Model:\")\n",
    "\n",
    "    # Convert lists of arrays to 2D NumPy arrays\n",
    "    raw_predictions = np.array(metric['raw_predictions'])\n",
    "    raw_targets = np.array(metric['raw_targets'])\n",
    "\n",
    "    plot_roc_curve(raw_predictions, raw_targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path(os.getcwd()).parent / 'models'\n",
    "# model_dir = os.path.join(os.getcwd(), 'models')\n",
    "# os.makedirs(model_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "\n",
    "def save_model(trained_model: Tuple[nn.Module, optim.Optimizer, nn.Module], path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the complete training state of a PyTorch model to a file.\n",
    "\n",
    "    This function saves the model's state dictionary, containing the learned\n",
    "    parameters (weights and biases), along with the state dictionaries of the\n",
    "    optimizer and the loss function. This allows you to load the entire training\n",
    "    state later and resume training from the saved point or use the model for\n",
    "    inference.\n",
    "\n",
    "    Args:\n",
    "        trained_model: A tuple containing the trained PyTorch model (nn.Module),\n",
    "                       optimizer (optim.Optimizer), and loss function (nn.Module).\n",
    "        path: The path to the file where the training state will be saved (str).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    model, optimizer, loss_function = trained_model\n",
    "    state_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss_function': loss_function.state_dict(),  # Save loss function instance (for class-based)\n",
    "    }\n",
    "    torch.save(state_dict, path)\n",
    "\n",
    "\n",
    "def load_model(model_definitions: Tuple[nn.Module, optim.Optimizer, nn.Module], path: str) -> Tuple[nn.Module, optim.Optimizer, nn.Module]:\n",
    "    \"\"\"\n",
    "    Loads a previously saved PyTorch model training state from a file.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the file where the training state is saved (str).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the loaded PyTorch model (nn.Module), optimizer (optim.Optimizer),\n",
    "        and loss function (nn.Module).\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"No model file found at {path}.\")\n",
    "\n",
    "    model, optimizer, loss_function = model_definitions\n",
    "    state_dict = torch.load(path)\n",
    "    model = model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer = optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    loss_function = loss_function.load_state_dict(state_dict['loss_function']) \n",
    "\n",
    "    return model, optimizer, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in trained_models.items():\n",
    "    print(f\"{name} Model:\")\n",
    "    print(model)\n",
    "    # Save the model using joblib\n",
    "    model_filename = os.path.join(model_dir, f\"{dl_selected_symbols.iloc[0]}_{name}.pth\")\n",
    "    save_model(model, model_filename)\n",
    "    print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "model_definitions = trained_models['BidirectionalLSTM']\n",
    "model_filename = os.path.join(model_dir, f\"{dl_selected_symbols.iloc[0]}_BidirectionalLSTM.pth\")\n",
    "\n",
    "trained_model = load_model(model_definitions, model_filename)\n",
    "trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the necessary libraries\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Initialize models\n",
    "# tree = DecisionTreeClassifier(max_depth=20)\n",
    "# svr = SVC(C=1.5)\n",
    "# lin = LogisticRegression()\n",
    "\n",
    "# # Initialize MetaTrader 5 or other data source\n",
    "# mt5.initialize()\n",
    "\n",
    "# # Symbols list from final_assets\n",
    "# symbols = final_assets[\"Symbol\"]\n",
    "\n",
    "# # To store results\n",
    "# results_list = []\n",
    "\n",
    "# # Iterate over all symbols\n",
    "# for symbol in tqdm(symbols):\n",
    "#     print(symbol, \"\\n\")\n",
    "#     try:\n",
    "#         # Retrieve data for the symbol\n",
    "#         df = get_data(symbol, 3500, mt5.TIMEFRAME_D1).dropna()\n",
    "        \n",
    "#         # Perform feature engineering\n",
    "#         processed_data = features_engineering(df)\n",
    "\n",
    "#         # Decision Tree\n",
    "#         sharpe_tree = predictor(processed_data, tree, reg=True)\n",
    "#         results_list.append([symbol, \"Tree\", sharpe_tree, len(df)])\n",
    "\n",
    "#         # # SVM (Support Vector Classifier)\n",
    "#         sharpe_svr = predictor(processed_data, svr, reg=False)\n",
    "#         results_list.append([symbol, \"SVR\", sharpe_svr, len(df)])\n",
    "\n",
    "#         # Logistic Regression\n",
    "#         sharpe_linreg = predictor(processed_data, lin, reg=False)\n",
    "#         results_list.append([symbol, \"LinReg\", sharpe_linreg, len(df)])\n",
    "\n",
    "#     except KeyError:\n",
    "#         continue\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Issue during data importation or processing for symbol {symbol}: {e}\")\n",
    "#         raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer as SklearnNormalizer\n",
    "\n",
    "# class Normalizer:\n",
    "#     \"\"\"\n",
    "#     A class for normalizing market data while preserving the original data.\n",
    "\n",
    "#     This class applies logarithm and a normalization (MinMax, StandardScale, Normalizer_l1, Normalizer_l2).\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, data):\n",
    "#         \"\"\"\n",
    "#         Initializes the Normalizer class with market data.\n",
    "\n",
    "#         Args:\n",
    "#             data (list[dict]): A list of dictionaries, where each dictionary represents a market\n",
    "#                                with keys like \"name\" and \"data\" (containing market data).\n",
    "#         \"\"\"\n",
    "#         self.__market_names = [e[\"name\"] for e in data]\n",
    "#         self.__original_data = [pd.DataFrame(e[\"data\"]) for e in data]  # Convert to DataFrame\n",
    "#         self.__normalized_data = None  # Stores normalized data after fit is called\n",
    "\n",
    "#     def __normalize(self, target, numerical_data, scaler_func):\n",
    "#         \"\"\"\n",
    "#         Normalizes data using the provided scaler function for specific target markets.\n",
    "\n",
    "#         Args:\n",
    "#             target (str): The target market(s) to normalize (\"all\" or a list of market names).\n",
    "#             numerical_data (list[pd.DataFrame]): A list of DataFrames containing numerical data for each market.\n",
    "#             scaler_func (callable): A function that performs data scaling (e.g., MinMaxScaler.fit_transform).\n",
    "\n",
    "#         Returns:\n",
    "#             list[pd.DataFrame]: List of normalized market DataFrames.\n",
    "#         \"\"\"\n",
    "#         normalized_markets = []\n",
    "#         target_markets = self.__market_names if target == \"all\" else target\n",
    "\n",
    "#         for market_name, market in zip(self.__market_names, numerical_data):\n",
    "#             if market_name in target_markets:\n",
    "#                 columns = market.columns\n",
    "#                 market_scaled = scaler_func(market)\n",
    "#                 normalized_markets.append(pd.DataFrame(market_scaled, columns=columns))\n",
    "\n",
    "#         return normalized_markets\n",
    "\n",
    "#     def fit(self, norm_type, features_list, target=\"all\"):\n",
    "#         \"\"\"\n",
    "#         Performs data normalization based on the specified type, features, and target market(s).\n",
    "\n",
    "#         Args:\n",
    "#             norm_type (str): The type of normalization to perform (\"MinMax\", \"StandardScale\", \"Normalizer_l1\", or \"Normalizer_l2\").\n",
    "#             features_list (list): A list of feature names to consider for normalization.\n",
    "#             target (str, optional): The target market(s) to normalize (\"all\" or a list of market names). Defaults to \"all\".\n",
    "\n",
    "#         Raises:\n",
    "#             ValueError: If an invalid normalization type is provided.\n",
    "#         \"\"\"\n",
    "#         # Extract numerical data for the specified features\n",
    "#         numerical_data = [market[features_list]._get_numeric_data() for market in self.__original_data]\n",
    "\n",
    "#         if norm_type == \"MinMax\":\n",
    "#             print(f\"Performing MinMax Normalization on {target}.\")\n",
    "#             scaler_func = MinMaxScaler().fit_transform\n",
    "#         elif norm_type == \"StandardScale\":\n",
    "#             print(f\"Performing StandardScale Normalization on {target}.\")\n",
    "#             scaler_func = StandardScaler().fit_transform\n",
    "#         elif norm_type.startswith(\"Normalizer\"):\n",
    "#             norm_value = norm_type.split(\"_\")[-1]  # Extract l1 or l2 from \"Normalizer_l1\" or \"Normalizer_l2\"\n",
    "#             if norm_value not in (\"l1\", \"l2\"):\n",
    "#                 raise ValueError(\"Invalid norm type for Normalizer. Must be 'l1' or 'l2'.\")\n",
    "#             print(f\"Performing Normalizer (norm={norm_value}) on {target}.\")\n",
    "#             scaler_func = SklearnNormalizer(norm=norm_value).fit_transform\n",
    "#         else:\n",
    "#             raise ValueError(f\"Invalid normalization type: {norm_type}\")\n",
    "\n",
    "#         self.__normalized_data = self.__normalize(target, numerical_data, scaler_func)\n",
    "\n",
    "#     def get_normalized_data(self, idx: int = None):\n",
    "#         \"\"\"\n",
    "#         Returns the normalized data if normalization has been performed, otherwise raises an error.\n",
    "        \n",
    "#         Args:\n",
    "#             idx (int, optional): The location/index of the normalized data. If None, returns all normalized data.\n",
    "\n",
    "#         Raises:\n",
    "#             RuntimeError: If data has not been normalized yet.\n",
    "#             IndexError: If the index is out of range.\n",
    "#         \"\"\"\n",
    "#         if self.__normalized_data is None:\n",
    "#             raise RuntimeError(\"Data has not been normalized yet. Please call 'fit' first.\")\n",
    "        \n",
    "#         if idx is not None:\n",
    "#             if idx < 0 or idx >= len(self.__normalized_data):\n",
    "#                 raise IndexError(\"Index out of range.\")\n",
    "#             return self.__normalized_data[idx]\n",
    "        \n",
    "#         return self.__normalized_data  # Return all normalized data if idx is None\n",
    "\n",
    "#     def get_original_data(self):\n",
    "#         \"\"\"\n",
    "#         Returns the original, un-normalized data.\n",
    "#         \"\"\"\n",
    "#         return [data.copy() for data in self.__original_data]  # Return a deep copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer as SklearnNormalizer\n",
    "\n",
    "# class Normalizer:\n",
    "#     \"\"\"\n",
    "#         A class for normalizing market data while preserving the original data.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, data):\n",
    "#         \"\"\"\n",
    "#             Initializes the Normalizer class with market data.\n",
    "\n",
    "#             Args:\n",
    "#                 data (list[dict]): A list of dictionaries, where each dictionary represents a market\n",
    "#                                    with keys like \"name\" and \"data\" (containing market data).\n",
    "#         \"\"\"\n",
    "#         self.__market_names = [e[\"name\"] for e in data]\n",
    "#         self.__original_data = [e[\"data\"] for e in data]  # TODO: Deep copy to preserve original data\n",
    "#         self.__normalized_data = None  # Stores normalized data after fit is called\n",
    "\n",
    "#     def __normalize(self, target, numerical_data, scaler_func):\n",
    "#         \"\"\"\n",
    "#             Normalizes data using the provided scaler function for specific target markets.\n",
    "\n",
    "#             Args:\n",
    "#                 target (str): The target market(s) to normalize (\"all\" or a list of market names).\n",
    "#                 numerical_data (list[pd.DataFrame]): A list of DataFrames containing numerical data for each market.\n",
    "#                 scaler_func (callable): A function that performs data scaling (e.g., MinMaxScaler.fit_transform).\n",
    "#         \"\"\"\n",
    "#         normalized_markets = []\n",
    "#         if target == \"all\":\n",
    "#             for market in numerical_data:\n",
    "#                 columns = market.columns\n",
    "#                 market_scaled = scaler_func(market)\n",
    "#                 market_scaled = pd.DataFrame(market_scaled, columns=columns)\n",
    "#                 normalized_markets.append(market_scaled)\n",
    "#         else:\n",
    "#             for market_name, market in zip(self.__market_names, numerical_data):\n",
    "#                 if market_name in target:\n",
    "#                     columns = market.columns\n",
    "#                     market_scaled = scaler_func(market)\n",
    "#                     market_scaled = pd.DataFrame(market_scaled, columns=columns)\n",
    "#                     normalized_markets.append(market_scaled)\n",
    "                    \n",
    "#         return normalized_markets\n",
    "\n",
    "#     def fit(self, norm_type, features_list, target=\"all\"):\n",
    "#         \"\"\"\n",
    "#             Performs data normalization based on the specified type, features, and target market(s).\n",
    "\n",
    "#             Args:\n",
    "#                 norm_type (str): The type of normalization to perform (\"MinMax\", \"StandardScale\", \"Normalizer_l1\", or \"Normalizer_l2\").\n",
    "#                 features_list (list): A list of feature names to consider for normalization.\n",
    "#                 target (str, optional): The target market(s) to normalize (\"all\" or a list of market names). Defaults to \"all\".\n",
    "\n",
    "#             Raises:\n",
    "#                 ValueError: If an invalid normalization type is provided.\n",
    "#         \"\"\"\n",
    "#         # Extract numerical data for the specified features\n",
    "#         numerical_data = [market[features_list]._get_numeric_data() for market in self.__original_data]\n",
    "\n",
    "#         if norm_type == \"MinMax\":\n",
    "#             print(f\"Performing MinMax Normalization on {target}.\")\n",
    "#             scaler_func = MinMaxScaler().fit_transform\n",
    "#             self.__normalized_data = self.__normalize(target, numerical_data, scaler_func)\n",
    "#         elif norm_type == \"StandardScale\":\n",
    "#             print(f\"Performing StandardScale Normalization on {target}.\")\n",
    "#             scaler_func = StandardScaler().fit_transform\n",
    "#             self.__normalized_data = self.__normalize(target, numerical_data, scaler_func)\n",
    "#         elif norm_type.startswith(\"Normalizer\"):\n",
    "#             norm_value = norm_type.split(\"_\")[-1]  # Extract l1 or l2 from \"Normalizer_l1\" or \"Normalizer_l2\"\n",
    "#             if norm_value not in (\"l1\", \"l2\"):\n",
    "#                 raise ValueError(\"Invalid norm type for Normalizer. Must be 'l1' or 'l2'.\")\n",
    "#             print(f\"Performing Normalizer (norm={norm_value}) on {target}.\")\n",
    "#             scaler_func = Normalizer(norm=norm_value).fit_transform\n",
    "#             self.__normalized_data = self.__normalize(target, numerical_data, scaler_func)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Invalid normalization type: {norm_type}\")\n",
    "\n",
    "#     def get_normalized_data(self, idx: int = 0):\n",
    "#         \"\"\"\n",
    "#             Returns the normalized data if normalization has been performed, otherwise raises an error.\n",
    "            \n",
    "#             Args:\n",
    "#                 idx (int): The location/index of the normalized data.\n",
    "\n",
    "#             Raises:\n",
    "#                 RuntimeError: If data has not been normalized yet.\n",
    "#                 IndexError: If the index is out of range.\n",
    "#         \"\"\"\n",
    "#         if self.__normalized_data is None:\n",
    "#             raise RuntimeError(\"Data has not been normalized yet. Please call 'fit' first.\")\n",
    "        \n",
    "#         if idx < 0 or idx >= len(self.__normalized_data):\n",
    "#             raise IndexError(\"Index out of range.\")\n",
    "        \n",
    "#         if idx is None:\n",
    "#             return self.__normalized_data\n",
    "        \n",
    "#         return self.__normalized_data[idx]\n",
    "\n",
    "#     def get_original_data(self):\n",
    "#         \"\"\"\n",
    "#             Returns the original, un-normalized data.\n",
    "#         \"\"\"\n",
    "#         return self.__original_data.copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, model_optimizer: str, model_loss: str, num_epochs: int = 20, dataloader: Optional[DataLoader] = None):\n",
    "#     \"\"\" \n",
    "#     Train Model\n",
    "#     \"\"\"\n",
    "#     # Optimizer\n",
    "#     optimizer_class = getattr(optim, model_optimizer.capitalize())\n",
    "#     optimizer = optimizer_class(model.parameters())\n",
    "\n",
    "#     # Loss function\n",
    "#     loss_function = getattr(nn, model_loss)()   # Use CrossEntropyLoss for multi-class classification\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for inputs, targets in dataloader:  # use DataLoader for batch loading\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = loss_function(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "#     return model, optimizer, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_train_test_set(x_train, x_test, y_train, y_test):\n",
    "#     \"\"\"Creates train and test datasets from numpy arrays or lists.\n",
    "\n",
    "#     Args:\n",
    "#         x_train: Training features.\n",
    "#         x_test: Test features.\n",
    "#         y_train: One-hot encoded training labels.\n",
    "#         y_test: One-hot encoded test labels.\n",
    "\n",
    "#     Returns:\n",
    "#         training_data: A TensorDataset for the training set.\n",
    "#         test_data: A TensorDataset for the test set.\n",
    "#     \"\"\"\n",
    "#     # Convert to torch tensors\n",
    "#     tensor_x_train = torch.Tensor(x_train)\n",
    "#     tensor_y_train = torch.argmax(torch.Tensor(y_train), dim=1).long()  # Convert one-hot to class indices\n",
    "#     training_data = TensorDataset(tensor_x_train, tensor_y_train)  # Create training dataset\n",
    "\n",
    "#     tensor_x_test = torch.Tensor(x_test)\n",
    "#     tensor_y_test = torch.argmax(torch.Tensor(y_test), dim=1).long()  # Convert one-hot to class indices\n",
    "#     test_data = TensorDataset(tensor_x_test, tensor_y_test)  # Create test dataset\n",
    "\n",
    "#     return training_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def predictor(data: dict, model, reg: bool = True, spread: float = 0.035, compressed_features: bool = True) -> float:\n",
    "#     \"\"\"\n",
    "#     Fits the model to the training data, makes predictions on the entire dataset, \n",
    "#     and computes the strategy's Sharpe ratio based on the predictions.\n",
    "    \n",
    "#     Args:\n",
    "#         data (dict): The output of the features_engineering function, containing the PCA-transformed datasets.\n",
    "#         model (object): The machine learning model to be used for prediction (e.g., classifier or regressor).\n",
    "#         reg (bool): If True, performs regression; otherwise, classification.\n",
    "#         spread (float): The transaction cost or spread to be considered in strategy returns.\n",
    "#         compressed_features (bool): If True, use PCA-transformed features; otherwise, use original features.\n",
    "\n",
    "#     Returns:\n",
    "#         float: The Sharpe ratio of the strategy based on predictions.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Extract the data from the dictionary\n",
    "#     X_train = data[\"X_train_pca\"] if compressed_features else data[\"X_train_scaled\"]\n",
    "#     X_test = data[\"X_test_pca\"] if compressed_features else data[\"X_test_scaled\"]\n",
    "#     X_val = data[\"X_val_pca\"] if compressed_features else data[\"X_val_scaled\"]\n",
    "#     y_train_reg = data[\"y_train_reg\"]\n",
    "#     y_train_cla = data[\"y_train_cla\"]\n",
    "#     df = data[\"processed_df\"]\n",
    "#     split_train_test = data[\"split_train_test\"]\n",
    "#     split_test_valid = data[\"split_test_valid\"]\n",
    "\n",
    "#     # Fit the model on the training data\n",
    "#     print(\"Model type:\", type(model).__name__)\n",
    "\n",
    "#     # Fit the model based on whether it is regression or classification\n",
    "#     if hasattr(model, 'fit'):\n",
    "#         # For sklearn models\n",
    "#         if 'sklearn' in str(type(model)):\n",
    "#             if reg:\n",
    "#                 model.fit(X_train, y_train_reg)\n",
    "#                 predictions = model.predict(np.concatenate((X_train, X_test, X_val), axis=0))\n",
    "#             else:\n",
    "#                 model.fit(X_train, y_train_cla)\n",
    "#                 predictions = model.predict(np.concatenate((X_train, X_test, X_val), axis=0))\n",
    "\n",
    "#         # For deep learning models (Pytorch, Keras, TensorFlow, etc.)\n",
    "#         elif hasattr(model, 'predict'):\n",
    "#             if reg:\n",
    "#                 model.fit(X_train, y_train_reg, epochs=100, verbose=0)  # Specify epochs as needed\n",
    "#                 predictions = model.predict(np.concatenate((X_train, X_test, X_val), axis=0)).flatten()\n",
    "#             else:\n",
    "#                 model.fit(X_train, y_train_cla, epochs=100, verbose=0)  # Specify epochs as needed\n",
    "#                 predictions = model.predict(np.concatenate((X_train, X_test, X_val), axis=0))\n",
    "\n",
    "#         else:\n",
    "#             raise ValueError(\"Model does not have a 'fit' or 'predict' method.\")\n",
    "#     else:\n",
    "#         raise ValueError(\"The provided model does not support fitting.\")\n",
    "\n",
    "#     # Clean the dataframe\n",
    "#     # df = df.dropna()\n",
    "#     # Impute missing values using IterativeImputer (Multiple Imputation by Chained Equations)\n",
    "#     imputer = IterativeImputer(random_state=0)\n",
    "#     clean_data = imputer.fit_transform(df)\n",
    "#     df = pd.DataFrame(clean_data, columns=data[\"processed_df\"].columns)\n",
    "\n",
    "\n",
    "#     # Convert classification predictions to -1 (sell) and 1 (buy)\n",
    "#     if not reg:\n",
    "#         predictions = np.where(predictions == 0, -1, 1)\n",
    "#         # predictions = np.argmax(predictions, axis=1)  # Assuming model returns probabilities\n",
    "\n",
    "#     # Add predictions to the dataframe\n",
    "#     df[\"prediction\"] = predictions\n",
    "\n",
    "#     # Compute the strategy returns (prediction * actual returns)\n",
    "#     df[\"strategy\"] = df[\"prediction\"] * df[\"returns\"]\n",
    "\n",
    "#     # Select strategy returns only for the test set period\n",
    "#     returns = df[\"strategy\"].iloc[split_train_test:split_test_valid]\n",
    "\n",
    "#     # Compute the Sharpe ratio of the strategy\n",
    "#     sharpe_ratio = np.sqrt(252) * (returns.mean() - (spread / 100)) / returns.std()\n",
    "\n",
    "#     return sharpe_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def predictor(data: dict, model, reg: bool = True, spread: float = 0.035, compressed_features: bool = True) -> float:\n",
    "#     \"\"\"\n",
    "#     Fits the model to the training data, makes predictions on the entire dataset, \n",
    "#     and computes the strategy's Sharpe ratio based on the predictions.\n",
    "    \n",
    "#     Args:\n",
    "#         data (dict): The output of the features_engineering function, containing the PCA-transformed datasets.\n",
    "#         model (object): The machine learning model to be used for prediction (e.g., classifier or regressor).\n",
    "#         reg (bool): If True, performs regression; otherwise, classification.\n",
    "#         spread (float): The transaction cost or spread to be considered in strategy returns.\n",
    "#         compressed_features (bool): If True, use PCA-transformed features; otherwise, use original features.\n",
    "\n",
    "#     Returns:\n",
    "#         float: The Sharpe ratio of the strategy based on predictions.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Extract the data from the dictionary\n",
    "#     X_train = data[\"X_train_pca\"] if compressed_features else data[\"X_train_scaled\"]\n",
    "#     X_test = data[\"X_test_pca\"] if compressed_features else data[\"X_test_scaled\"]\n",
    "#     X_val = data[\"X_val_pca\"] if compressed_features else data[\"X_val_scaled\"]\n",
    "#     y_train_reg = data[\"y_train_reg\"]\n",
    "#     y_train_cla = data[\"y_train_cla\"]\n",
    "#     df = data[\"processed_df\"]\n",
    "#     split_train_test = data[\"split_train_test\"]\n",
    "#     split_test_valid = data[\"split_test_valid\"]\n",
    "\n",
    "#     # Fit the model on the training data\n",
    "#     print(\"Model type:\", type(model).__name__)\n",
    "\n",
    "#     # Fit the model on the training data\n",
    "#     model.fit(X_train, y_train_cla)\n",
    "\n",
    "#     # Clean the dataframe\n",
    "#     # df = df.dropna()\n",
    "#     # Impute missing values using IterativeImputer (Multiple Imputation by Chained Equations)\n",
    "#     imputer = IterativeImputer(random_state=0)\n",
    "#     clean_data = imputer.fit_transform(df)\n",
    "#     df = pd.DataFrame(clean_data, columns=data[\"processed_df\"].columns)\n",
    "    \n",
    "#     # Create predictions for the concatenated dataset (train, test, validation)\n",
    "#     predictions = model.predict(np.concatenate((X_train, X_test, X_val), axis=0))\n",
    "\n",
    "#     # Convert classification predictions to -1 (sell) and 1 (buy)\n",
    "#     if not reg:\n",
    "#         predictions = np.where(predictions == 0, -1, 1)\n",
    "#         # predictions = np.argmax(predictions, axis=1)  # Assuming model returns probabilities\n",
    "\n",
    "#     # Add predictions to the dataframe\n",
    "#     df[\"prediction\"] = predictions\n",
    "\n",
    "#     # Compute the strategy returns (prediction * actual returns)\n",
    "#     df[\"strategy\"] = df[\"prediction\"] * df[\"returns\"]\n",
    "\n",
    "#     # Select strategy returns only for the test set period\n",
    "#     returns = df[\"strategy\"].iloc[split_train_test:split_test_valid]\n",
    "\n",
    "#     # Compute the Sharpe ratio of the strategy\n",
    "#     sharpe_ratio = np.sqrt(252) * (returns.mean() - (spread / 100)) / returns.std()\n",
    "\n",
    "#     return sharpe_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the necessary libraries\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from tqdm import tqdm\n",
    "# import concurrent.futures\n",
    "\n",
    "# # Initialize models (only once)\n",
    "# tree = DecisionTreeClassifier(max_depth=20)\n",
    "# svr = SVC(C=1.5)\n",
    "# lin = LogisticRegression()\n",
    "\n",
    "# # Symbols list from final_assets\n",
    "# symbols = final_assets[\"Symbol\"]\n",
    "\n",
    "# # To store results\n",
    "# results_list = []\n",
    "\n",
    "# # Define the function for processing each symbol\n",
    "# def process_symbol(symbol):\n",
    "#     print(symbol)\n",
    "#     try:\n",
    "#         # Retrieve data for the symbol\n",
    "#         df = get_data(symbol, 3500, mt5.TIMEFRAME_D1).dropna()\n",
    "\n",
    "#         # Perform feature engineering\n",
    "#         processed_data = features_engineering(df)\n",
    "\n",
    "#         # Decision Tree\n",
    "#         sharpe_tree = predictor(processed_data, tree, reg=True, spread=spread_threshold)\n",
    "#         result_tree = [symbol, \"Tree\", sharpe_tree, len(df)]\n",
    "\n",
    "#         # SVM (Support Vector Classifier)\n",
    "#         sharpe_svr = predictor(processed_data, svr, reg=False, spread=spread_threshold)\n",
    "#         result_svr = [symbol, \"SVR\", sharpe_svr, len(df)]\n",
    "\n",
    "#         # Logistic Regression\n",
    "#         sharpe_linreg = predictor(processed_data, lin, reg=False, spread=spread_threshold)\n",
    "#         result_linreg = [symbol, \"LinReg\", sharpe_linreg, len(df)]\n",
    "\n",
    "#         return [result_tree, result_svr, result_linreg]\n",
    "\n",
    "#     except KeyError:\n",
    "#         return []\n",
    "#     except Exception as e:\n",
    "#         print(f\"Issue during data importation or processing for symbol {symbol}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# # Use ThreadPoolExecutor to parallelize the process\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     # Execute the process for all symbols in parallel\n",
    "#     results = list(tqdm(executor.map(process_symbol, symbols), total=len(symbols)))\n",
    "\n",
    "# # Flatten the results list and filter out any empty results\n",
    "# results_list = [result for sublist in results for result in sublist if sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the results after processing all symbols\n",
    "# for result in results_list:\n",
    "#     print(f\"Symbol: {result[0]}, Model: {result[1]}, Sharpe Ratio: {result[2]:.4f}, Data Points: {result[3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame(results_list, columns=[\"Symbol\", \"Model\", \"Sharpe\", \"Length\"])\n",
    "\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 35 Symbols/Pairs\n",
    "# results.sort_values(by=\"Sharpe\", ascending=False).loc[results[\"Length\"]>600].head(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the algorithms (Ensemble Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of symbols to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 5 Symbols/Pairs | [\"US2000\", \"Bitcoin\", \"AUDUSD\", \"NAS100\", \"US500\"]\n",
    "# top_symbols = set(results.sort_values(by=\"Sharpe\", ascending=False).loc[results[\"Length\"]>600]['Symbol'].to_list()[:7])\n",
    "\n",
    "# top_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "# from pathlib import Path\n",
    "# from joblib import dump\n",
    "# from sklearn.ensemble import VotingRegressor, VotingClassifier\n",
    "# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "# from sklearn.svm import SVR, SVC\n",
    "# from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def voting(df, reg=True):\n",
    "#     \"\"\"Create a strategy using a voting method.\"\"\"\n",
    "\n",
    "#     processed_data = features_engineering(df)\n",
    "\n",
    "#     # Extract the data from the dictionary\n",
    "#     X_train_pca = processed_data[\"X_train_pca\"]\n",
    "#     X_test_pca = processed_data[\"X_test_pca\"]\n",
    "#     X_val_pca = processed_data[\"X_val_pca\"]\n",
    "#     y_train_reg = processed_data[\"y_train_reg\"]\n",
    "#     y_train_cla = processed_data[\"y_train_cla\"]\n",
    "    \n",
    "#     # Initialize the models\n",
    "#     if reg:\n",
    "#         tree = DecisionTreeRegressor(max_depth=6)\n",
    "#         svr = SVR(epsilon=1.5)\n",
    "#         lin = LinearRegression()\n",
    "#         vot = VotingRegressor(estimators=[\n",
    "#             ('lr', lin), (\"tree\", tree), (\"svr\", svr)])\n",
    "#     else:\n",
    "#         tree = DecisionTreeClassifier(max_depth=6)\n",
    "#         svr = SVC()\n",
    "#         lin = LogisticRegression()\n",
    "\n",
    "#         vot = VotingClassifier(estimators=[\n",
    "#             ('lr', lin), (\"tree\", tree), (\"svr\", svr)])\n",
    "\n",
    "#     # Train the model based on regression or classification task\n",
    "#     if reg:\n",
    "#         vot.fit(X_train_pca, y_train_reg)\n",
    "#     else:\n",
    "#         vot.fit(X_train_pca, y_train_cla)\n",
    "\n",
    "#     # Remove missing values\n",
    "#     # df = df.dropna()\n",
    "#     # Impute missing values using IterativeImputer (Multiple Imputation by Chained Equations)\n",
    "#     imputer = IterativeImputer(random_state=0)\n",
    "#     clean_data = imputer.fit_transform(df)\n",
    "#     df = pd.DataFrame(clean_data, columns=processed_data[\"processed_df\"].columns)\n",
    "\n",
    "#     # Create predictions for the entire dataset\n",
    "#     df[\"prediction\"] = vot.predict(np.concatenate((X_train_pca, X_test_pca, X_val_pca), axis=0))\n",
    "\n",
    "#     # In case of classification, map the predictions to -1 and 1\n",
    "#     if not reg:\n",
    "#         df[\"prediction\"] = np.where(df[\"prediction\"] == 0, -1, 1)\n",
    "\n",
    "#     # Compute strategy based on predictions\n",
    "#     df[\"strategy\"] = np.sign(df[\"prediction\"]) * df[\"returns\"]\n",
    "#     df[\"low_strategy\"] = np.where(df[\"prediction\"] > 0, df[\"sLow\"], -df[\"sHigh\"])\n",
    "#     df[\"high_strategy\"] = np.where(df[\"prediction\"] > 0, df[\"sHigh\"], -df[\"sLow\"])\n",
    "\n",
    "#     return vot, df[\"strategy\"], df[\"low_strategy\"], df[\"high_strategy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for models if it doesn't exist\n",
    "model_dir = Path(os.getcwd()).parent / 'models'\n",
    "# model_dir = os.path.join(os.getcwd(), 'models')\n",
    "# os.makedirs(model_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = pd.DataFrame()\n",
    "low_assets = pd.DataFrame()\n",
    "high_assets = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Function to compute returns and other metrics\n",
    "def compute_metrics(df):\n",
    "    \"\"\" Create custom metrics for strategy returns. \"\"\"\n",
    "    df[\"returns\"] = ((df[\"close\"] - df[\"close\"].shift(1)) / df[\"close\"])\n",
    "    df[\"sLow\"] = ((df[\"low\"] - df[\"close\"].shift(1)) / df[\"close\"].shift(1))\n",
    "    df[\"sHigh\"] = ((df[\"high\"] - df[\"close\"].shift(1)) / df[\"close\"].shift(1))\n",
    "    return df.dropna()  # Remove missing values\n",
    "\n",
    "# Function to load and process data\n",
    "def get_and_process_data(symbol):\n",
    "    \"\"\" Load data and apply feature engineering. \"\"\"\n",
    "    df = get_data(symbol, 3500, mt5.TIMEFRAME_D1).dropna()\n",
    "    df = compute_metrics(df)  # Compute metrics\n",
    "    return df\n",
    "\n",
    "\n",
    "# for symbol in top_symbols:\n",
    "#     print(f\"Processing {symbol}...\")\n",
    "    \n",
    "#     # Load and process the data\n",
    "#     df = get_and_process_data(symbol)\n",
    "    \n",
    "#     # Compute the strategy using the voting function\n",
    "#     vot, results[symbol], low_assets[symbol], high_assets[symbol] = voting(df, reg=False)\n",
    "\n",
    "#     # Save the model using joblib\n",
    "#     model_filename = os.path.join(model_dir, f\"{symbol}_voting.joblib\")\n",
    "#     dump(vot, model_filename)\n",
    "#     print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "# # Shutdown MT5\n",
    "# mt5.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo_trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
