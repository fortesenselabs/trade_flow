{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install TradeFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!python3 -m pip install git+https://github.com/fortesenselabs/trade_flow.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_steps = 1000\n",
        "n_episodes = 20\n",
        "window_size = 30\n",
        "memory_capacity = n_steps * 10\n",
        "save_path = 'agents/'\n",
        "n_bins = 5             # Number of bins to partition the dataset evenly in order to evaluate class sparsity.\n",
        "seed = 1337"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Data Fetching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_225932/3093378899.py:5: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
            "  pd.options.mode.use_inf_as_na = True\n"
          ]
        }
      ],
      "source": [
        "from trade_flow.adapters.cdd import CryptoDataDownload\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.use_inf_as_na = True\n",
        "\n",
        "def prepare_data(df):\n",
        "    df['volume'] = np.int64(df['volume'])\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df.sort_values(by='date', ascending=True, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df['date'] = df['date'].dt.strftime('%Y-%m-%d %I:%M %p')\n",
        "    return df\n",
        "\n",
        "def fetch_data():\n",
        "    cdd = CryptoDataDownload()\n",
        "    bitfinex_data = cdd.fetch(\"Bitfinex\", \"USD\", \"BTC\", \"1h\")\n",
        "    bitfinex_data = bitfinex_data[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
        "    bitfinex_data = prepare_data(bitfinex_data)\n",
        "    return bitfinex_data\n",
        "\n",
        "def load_csv(filename):\n",
        "    df = pd.read_csv('data/' + filename, skiprows=1)\n",
        "    df.drop(columns=['symbol', 'volume_btc'], inplace=True)\n",
        "\n",
        "    # Fix timestamp from \"2019-10-17 09-AM\" to \"2019-10-17 09-00-00 AM\"\n",
        "    df['date'] = df['date'].str[:14] + '00-00 ' + df['date'].str[-2:]\n",
        "\n",
        "    return prepare_data(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-05-15 06:00 AM</td>\n",
              "      <td>8723.8</td>\n",
              "      <td>8793.0</td>\n",
              "      <td>8714.9</td>\n",
              "      <td>8739.0</td>\n",
              "      <td>8988053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-05-15 07:00 AM</td>\n",
              "      <td>8739.0</td>\n",
              "      <td>8754.8</td>\n",
              "      <td>8719.3</td>\n",
              "      <td>8743.0</td>\n",
              "      <td>2288904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-05-15 08:00 AM</td>\n",
              "      <td>8743.0</td>\n",
              "      <td>8743.1</td>\n",
              "      <td>8653.2</td>\n",
              "      <td>8723.7</td>\n",
              "      <td>8891773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-05-15 09:00 AM</td>\n",
              "      <td>8723.7</td>\n",
              "      <td>8737.8</td>\n",
              "      <td>8701.2</td>\n",
              "      <td>8708.1</td>\n",
              "      <td>2054868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-05-15 10:00 AM</td>\n",
              "      <td>8708.1</td>\n",
              "      <td>8855.7</td>\n",
              "      <td>8695.8</td>\n",
              "      <td>8784.4</td>\n",
              "      <td>17309722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55394</th>\n",
              "      <td>2024-09-08 09:00 PM</td>\n",
              "      <td>54525.0</td>\n",
              "      <td>54738.0</td>\n",
              "      <td>54483.0</td>\n",
              "      <td>54603.0</td>\n",
              "      <td>1984310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55395</th>\n",
              "      <td>2024-09-08 10:00 PM</td>\n",
              "      <td>54604.0</td>\n",
              "      <td>55448.0</td>\n",
              "      <td>54525.0</td>\n",
              "      <td>54678.0</td>\n",
              "      <td>5234089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55396</th>\n",
              "      <td>2024-09-08 11:00 PM</td>\n",
              "      <td>54673.0</td>\n",
              "      <td>55123.0</td>\n",
              "      <td>54608.0</td>\n",
              "      <td>55011.0</td>\n",
              "      <td>4444903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55397</th>\n",
              "      <td>2024-09-09 12:00 AM</td>\n",
              "      <td>55019.0</td>\n",
              "      <td>55369.0</td>\n",
              "      <td>54854.0</td>\n",
              "      <td>55213.0</td>\n",
              "      <td>2759714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55398</th>\n",
              "      <td>2024-09-09 01:00 AM</td>\n",
              "      <td>55194.0</td>\n",
              "      <td>55500.0</td>\n",
              "      <td>55194.0</td>\n",
              "      <td>55295.0</td>\n",
              "      <td>1271660</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>55399 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      date     open     high      low    close    volume\n",
              "0      2018-05-15 06:00 AM   8723.8   8793.0   8714.9   8739.0   8988053\n",
              "1      2018-05-15 07:00 AM   8739.0   8754.8   8719.3   8743.0   2288904\n",
              "2      2018-05-15 08:00 AM   8743.0   8743.1   8653.2   8723.7   8891773\n",
              "3      2018-05-15 09:00 AM   8723.7   8737.8   8701.2   8708.1   2054868\n",
              "4      2018-05-15 10:00 AM   8708.1   8855.7   8695.8   8784.4  17309722\n",
              "...                    ...      ...      ...      ...      ...       ...\n",
              "55394  2024-09-08 09:00 PM  54525.0  54738.0  54483.0  54603.0   1984310\n",
              "55395  2024-09-08 10:00 PM  54604.0  55448.0  54525.0  54678.0   5234089\n",
              "55396  2024-09-08 11:00 PM  54673.0  55123.0  54608.0  55011.0   4444903\n",
              "55397  2024-09-09 12:00 AM  55019.0  55369.0  54854.0  55213.0   2759714\n",
              "55398  2024-09-09 01:00 AM  55194.0  55500.0  55194.0  55295.0   1271660\n",
              "\n",
              "[55399 rows x 6 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = fetch_data()\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create features for the feed module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import ta as ta1\n",
        "import pandas_ta as ta\n",
        "\n",
        "import quantstats as qs\n",
        "qs.extend_pandas()\n",
        "\n",
        "def fix_dataset_inconsistencies(dataframe, fill_value=None):\n",
        "    dataframe = dataframe.replace([-np.inf, np.inf], np.nan)\n",
        "\n",
        "    # This is done to avoid filling middle holes with backfilling.\n",
        "    if fill_value is None:\n",
        "        dataframe.iloc[0,:] = \\\n",
        "            dataframe.apply(lambda column: column.iloc[column.first_valid_index()], axis='index')\n",
        "    else:\n",
        "        dataframe.iloc[0,:] = \\\n",
        "            dataframe.iloc[0,:].fillna(fill_value)\n",
        "\n",
        "    return dataframe.fillna(axis='index', method='pad').dropna(axis='columns')\n",
        "\n",
        "def rsi(price: 'pd.Series[pd.Float64Dtype]', period: float) -> 'pd.Series[pd.Float64Dtype]':\n",
        "    r = price.diff()\n",
        "    upside = r.clip(lower=0)\n",
        "    downside = -r.clip(upper=0)\n",
        "    rs = upside.ewm(alpha=1 / period).mean() / downside.ewm(alpha=1 / period).mean()\n",
        "    return 100*(1 - (1 + rs) ** -1)\n",
        "\n",
        "def macd(price: 'pd.Series[pd.Float64Dtype]', fast: float, slow: float, signal: float) -> 'pd.Series[pd.Float64Dtype]':\n",
        "    fm = price.ewm(span=fast, adjust=False).mean()\n",
        "    sm = price.ewm(span=slow, adjust=False).mean()\n",
        "    md = fm - sm\n",
        "    signal = md - md.ewm(span=signal, adjust=False).mean()\n",
        "    return signal\n",
        "\n",
        "def generate_all_default_quantstats_features(data):\n",
        "    excluded_indicators = [\n",
        "        'compare',\n",
        "        'greeks',\n",
        "        'information_ratio',\n",
        "        'omega',\n",
        "        'r2',\n",
        "        'r_squared',\n",
        "        'rolling_greeks',\n",
        "        'warn',\n",
        "    ]\n",
        "    \n",
        "    indicators_list = [f for f in dir(qs.stats) if f[0] != '_' and f not in excluded_indicators]\n",
        "    \n",
        "    df = data.copy()\n",
        "    df = df.set_index('date')\n",
        "    df.index = pd.DatetimeIndex(df.index)\n",
        "\n",
        "    for indicator_name in indicators_list:\n",
        "        try:\n",
        "            #print(indicator_name)\n",
        "            indicator = qs.stats.__dict__[indicator_name](df['close'])\n",
        "            if isinstance(indicator, pd.Series):\n",
        "                indicator = indicator.to_frame(name=indicator_name)\n",
        "                df = pd.concat([df, indicator], axis='columns')\n",
        "        except (pd.errors.InvalidIndexError, ValueError):\n",
        "            pass\n",
        "\n",
        "    df = df.reset_index()\n",
        "    return df\n",
        "\n",
        "def generate_features(data):\n",
        "    # Automatically-generated using pandas_ta\n",
        "    df = data.copy()\n",
        "\n",
        "    strategies = ['candles', \n",
        "                  'cycles', \n",
        "                  'momentum', \n",
        "                  'overlap', \n",
        "                  'performance', \n",
        "                  'statistics', \n",
        "                  'trend', \n",
        "                  'volatility', \n",
        "                  'volume']\n",
        "\n",
        "    df.index = pd.DatetimeIndex(df.index)\n",
        "\n",
        "    cores = os.cpu_count()\n",
        "    df.ta.cores = cores\n",
        "\n",
        "    for strategy in strategies:\n",
        "        df.ta.study(strategy, exclude=['kvo'])\n",
        "\n",
        "    df = df.set_index('date')\n",
        "\n",
        "    # Generate all default indicators from ta library\n",
        "    ta1.add_all_ta_features(data, \n",
        "                            'open', \n",
        "                            'high', \n",
        "                            'low', \n",
        "                            'close', \n",
        "                            'volume', \n",
        "                            fillna=True)\n",
        "\n",
        "    # Naming convention across most technical indicator libraries\n",
        "    data = data.rename(columns={'open': 'Open', \n",
        "                                'high': 'High', \n",
        "                                'low': 'Low', \n",
        "                                'close': 'Close', \n",
        "                                'volume': 'Volume'})\n",
        "    data = data.set_index('date')\n",
        "\n",
        "    # Custom indicators\n",
        "    features = pd.DataFrame.from_dict({\n",
        "        'prev_open': data['Open'].shift(1),\n",
        "        'prev_high': data['High'].shift(1),\n",
        "        'prev_low': data['Low'].shift(1),\n",
        "        'prev_close': data['Close'].shift(1),\n",
        "        'prev_volume': data['Volume'].shift(1),\n",
        "        'vol_5': data['Close'].rolling(window=5).std().abs(),\n",
        "        'vol_10': data['Close'].rolling(window=10).std().abs(),\n",
        "        'vol_20': data['Close'].rolling(window=20).std().abs(),\n",
        "        'vol_30': data['Close'].rolling(window=30).std().abs(),\n",
        "        'vol_50': data['Close'].rolling(window=50).std().abs(),\n",
        "        'vol_60': data['Close'].rolling(window=60).std().abs(),\n",
        "        'vol_100': data['Close'].rolling(window=100).std().abs(),\n",
        "        'vol_200': data['Close'].rolling(window=200).std().abs(),\n",
        "        'ma_5': data['Close'].rolling(window=5).mean(),\n",
        "        'ma_10': data['Close'].rolling(window=10).mean(),\n",
        "        'ma_20': data['Close'].rolling(window=20).mean(),\n",
        "        'ma_30': data['Close'].rolling(window=30).mean(),\n",
        "        'ma_50': data['Close'].rolling(window=50).mean(),\n",
        "        'ma_60': data['Close'].rolling(window=60).mean(),\n",
        "        'ma_100': data['Close'].rolling(window=100).mean(),\n",
        "        'ma_200': data['Close'].rolling(window=200).mean(),\n",
        "        'ema_5': ta1.trend.ema_indicator(data['Close'], window=5, fillna=True),\n",
        "        'ema_10': ta1.trend.ema_indicator(data['Close'], window=10, fillna=True),\n",
        "        'ema_20': ta1.trend.ema_indicator(data['Close'], window=20, fillna=True),\n",
        "        'ema_60': ta1.trend.ema_indicator(data['Close'], window=60, fillna=True),\n",
        "        'ema_64': ta1.trend.ema_indicator(data['Close'], window=64, fillna=True),\n",
        "        'ema_120': ta1.trend.ema_indicator(data['Close'], window=120, fillna=True),\n",
        "        'lr_open': np.log(data['Open']).diff().fillna(0),\n",
        "        'lr_high': np.log(data['High']).diff().fillna(0),\n",
        "        'lr_low': np.log(data['Low']).diff().fillna(0),\n",
        "        'lr_close': np.log(data['Close']).diff().fillna(0),\n",
        "        'r_volume': data['Close'].diff().fillna(0),\n",
        "        'rsi_5': rsi(data['Close'], period=5),\n",
        "        'rsi_10': rsi(data['Close'], period=10),\n",
        "        'rsi_100': rsi(data['Close'], period=100),\n",
        "        'rsi_7': rsi(data['Close'], period=7),\n",
        "        'rsi_28': rsi(data['Close'], period=28),\n",
        "        'rsi_6': rsi(data['Close'], period=6),\n",
        "        'rsi_14': rsi(data['Close'], period=14),\n",
        "        'rsi_26': rsi(data['Close'], period=24),\n",
        "        'macd_normal': macd(data['Close'], fast=12, slow=26, signal=9),\n",
        "        'macd_short': macd(data['Close'], fast=10, slow=50, signal=5),\n",
        "        'macd_long': macd(data['Close'], fast=200, slow=100, signal=50),\n",
        "    })\n",
        "\n",
        "    # Concatenate both manually and automatically generated features\n",
        "    data = pd.concat([data, features], axis='columns').fillna(method='pad')\n",
        "\n",
        "    # Remove potential column duplicates\n",
        "    data = data.loc[:,~data.columns.duplicated()]\n",
        "\n",
        "    # Revert naming convention\n",
        "    data = data.rename(columns={'Open': 'open', \n",
        "                                'High': 'high', \n",
        "                                'Low': 'low', \n",
        "                                'Close': 'close', \n",
        "                                'Volume': 'volume'})\n",
        "\n",
        "    # Concatenate both manually and automatically generated features\n",
        "    data = pd.concat([data, df], axis='columns').fillna(method='pad')\n",
        "\n",
        "    # Remove potential column duplicates\n",
        "    data = data.loc[:,~data.columns.duplicated()]\n",
        "\n",
        "    data = data.reset_index()\n",
        "\n",
        "    # Generate all default quantstats features\n",
        "    df_quantstats = generate_all_default_quantstats_features(data)\n",
        "\n",
        "    # Concatenate both manually and automatically generated features\n",
        "    data = pd.concat([data, df_quantstats], axis='columns').fillna(method='pad')\n",
        "\n",
        "    # Remove potential column duplicates\n",
        "    data = data.loc[:,~data.columns.duplicated()]\n",
        "\n",
        "    # A lot of indicators generate NaNs at the beginning of DataFrames, so remove them\n",
        "    data = data.iloc[200:]\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    data = fix_dataset_inconsistencies(data, fill_value=None)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'AnalysisIndicators' object has no attribute 'study'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data\n",
            "Cell \u001b[0;32mIn[6], line 87\u001b[0m, in \u001b[0;36mgenerate_features\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     84\u001b[0m df\u001b[38;5;241m.\u001b[39mta\u001b[38;5;241m.\u001b[39mcores \u001b[38;5;241m=\u001b[39m cores\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m strategy \u001b[38;5;129;01min\u001b[39;00m strategies:\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudy\u001b[49m(strategy, exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkvo\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     89\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Generate all default indicators from ta library\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'AnalysisIndicators' object has no attribute 'study'"
          ]
        }
      ],
      "source": [
        "data = generate_features(data)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove features with low variance before splitting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
        "date = data[['date']].copy()\n",
        "data = data.drop(columns=['date'])\n",
        "sel.fit(data)\n",
        "data[data.columns[sel.get_support(indices=True)]]\n",
        "data = pd.concat([date, data], axis='columns')\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_data(data):\n",
        "    X = data.copy()\n",
        "    y = X['close'].pct_change()\n",
        "\n",
        "    X_train_test, X_valid, y_train_test, y_valid = \\\n",
        "        train_test_split(data, data['close'].pct_change(), train_size=0.67, test_size=0.33, shuffle=False)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        train_test_split(X_train_test, y_train_test, train_size=0.50, test_size=0.50, shuffle=False)\n",
        "\n",
        "    return X_train, X_test, X_valid, y_train, y_test, y_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, X_valid, y_train, y_test, y_valid = \\\n",
        "    split_data(data)\n",
        "\n",
        "import os\n",
        "cwd = os.getcwd()\n",
        "train_csv = os.path.join(cwd, 'train.csv')\n",
        "test_csv = os.path.join(cwd, 'test.csv')\n",
        "valid_csv = os.path.join(cwd, 'valid.csv')\n",
        "X_train.to_csv(train_csv, index=False)\n",
        "X_test.to_csv(test_csv, index=False)\n",
        "X_valid.to_csv(valid_csv, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get dataset statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import iqr\n",
        "\n",
        "def estimate_outliers(data):\n",
        "    return iqr(data) * 1.5\n",
        "\n",
        "def estimate_percent_gains(data, column='close'):\n",
        "    returns = get_returns(data, column=column)\n",
        "    gains = estimate_outliers(returns)\n",
        "    return gains\n",
        "\n",
        "def get_returns(data, column='close'):\n",
        "    return fix_dataset_inconsistencies(data[[column]].pct_change(), fill_value=0)\n",
        "\n",
        "def precalculate_ground_truths(data, column='close', threshold=None):\n",
        "    returns = get_returns(data, column=column)\n",
        "    gains = estimate_outliers(returns) if threshold is None else threshold\n",
        "    binary_gains = (returns[column] > gains).astype(int)\n",
        "    return binary_gains\n",
        "\n",
        "def is_null(data):\n",
        "    return data.isnull().sum().sum() > 0\n",
        "\n",
        "def is_sparse(data, column='close'):\n",
        "    binary_gains = precalculate_ground_truths(data, column=column)\n",
        "    bins = [n * (binary_gains.shape[0] // n_bins) for n in range(n_bins)]\n",
        "    bins += [binary_gains.shape[0]]\n",
        "    bins = [binary_gains.iloc[bins[n]:bins[n + 1]] for n in range(n_bins)]\n",
        "    return all([bin.astype(bool).any() for bin in bins])\n",
        "\n",
        "def is_data_predictible(data, column):\n",
        "    return not is_null(data) & is_sparse(data, column)\n",
        "\n",
        "data.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate outlier sparsity of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(get_returns(data, column='close'))\n",
        "plt.show()\n",
        "is_data_predictible(data, 'close')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Percentage of the dataset generating rewards (keep between 5% to 15% or just rely on is_data_predictible())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(precalculate_ground_truths(data, column='close').iloc[:1000])\n",
        "plt.show()\n",
        "percent_rewardable = str(round(100 + precalculate_ground_truths(data, column='close').value_counts().pct_change().iloc[-1] * 100, 2)) + '%'\n",
        "print(percent_rewardable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold to pass to AnomalousProfit reward scheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_test = pd.concat([X_train, X_test], axis='index')\n",
        "#threshold = estimate_percent_gains(X_train_test, 'close')\n",
        "threshold = estimate_percent_gains(X_train, 'close')\n",
        "threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implement basic feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from feature_engine.selection import SelectBySingleFeaturePerformance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, \n",
        "                            random_state=seed, \n",
        "                            n_jobs=6)\n",
        "\n",
        "sel = SelectBySingleFeaturePerformance(variables=None, \n",
        "                                       estimator=rf, \n",
        "                                       scoring=\"roc_auc\", \n",
        "                                       cv=5, \n",
        "                                       threshold=0.5)\n",
        "\n",
        "sel.fit(X_train, precalculate_ground_truths(X_train, column='close'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_performance = pd.Series(sel.feature_performance_).sort_values(ascending=False)\n",
        "feature_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_performance.plot.bar(figsize=(20, 5))\n",
        "plt.title('Performance of ML models trained with individual features')\n",
        "plt.ylabel('roc-auc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_to_drop = sel.features_to_drop_\n",
        "features_to_drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_drop = list(set(features_to_drop) - set(['open', 'high', 'low', 'close', 'volume']))\n",
        "len(to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train.drop(columns=to_drop)\n",
        "X_test = X_test.drop(columns=to_drop)\n",
        "X_valid = X_valid.drop(columns=to_drop)\n",
        "\n",
        "X_train.shape, X_test.shape, X_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize the dataset subsets to make the model converge faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
        "\n",
        "scaler_type = MinMaxScaler\n",
        "\n",
        "def get_feature_scalers(X, scaler_type=scaler_type):\n",
        "    scalers = []\n",
        "    for name in list(X.columns[X.columns != 'date']):\n",
        "        scalers.append(scaler_type().fit(X[name].values.reshape(-1, 1)))\n",
        "    return scalers\n",
        "\n",
        "def get_scaler_transforms(X, scalers):\n",
        "    X_scaled = []\n",
        "    for name, scaler in zip(list(X.columns[X.columns != 'date']), scalers):\n",
        "        X_scaled.append(scaler.transform(X[name].values.reshape(-1, 1)))\n",
        "    X_scaled = pd.concat([pd.DataFrame(column, columns=[name]) for name, column in \\\n",
        "                          zip(list(X.columns[X.columns != 'date']), X_scaled)], axis='columns')\n",
        "    return X_scaled\n",
        "\n",
        "def normalize_data(X_train, X_test, X_valid):\n",
        "    X_train_test = pd.concat([X_train, X_test], axis='index')\n",
        "    X_train_test_valid = pd.concat([X_train_test, X_valid], axis='index')\n",
        "\n",
        "    X_train_test_dates = X_train_test[['date']]\n",
        "    X_train_test_valid_dates = X_train_test_valid[['date']]\n",
        "\n",
        "    X_train_test = X_train_test.drop(columns=['date'])\n",
        "    X_train_test_valid = X_train_test_valid.drop(columns=['date'])\n",
        "\n",
        "    train_test_scalers = \\\n",
        "        get_feature_scalers(X_train_test, \n",
        "                            scaler_type=scaler_type)\n",
        "    train_test_valid_scalers = \\\n",
        "        get_feature_scalers(X_train_test_valid, \n",
        "                            scaler_type=scaler_type)\n",
        "\n",
        "    X_train_test_scaled = \\\n",
        "        get_scaler_transforms(X_train_test, \n",
        "                              train_test_scalers)\n",
        "    X_train_test_valid_scaled = \\\n",
        "        get_scaler_transforms(X_train_test_valid, \n",
        "                              train_test_scalers)\n",
        "    X_train_test_valid_scaled_leaking = \\\n",
        "        get_scaler_transforms(X_train_test_valid, \n",
        "                              train_test_valid_scalers)\n",
        "\n",
        "    X_train_test_scaled = \\\n",
        "        pd.concat([X_train_test_dates, \n",
        "                   X_train_test_scaled], \n",
        "                  axis='columns')\n",
        "    X_train_test_valid_scaled = \\\n",
        "        pd.concat([X_train_test_valid_dates, \n",
        "                   X_train_test_valid_scaled], \n",
        "                  axis='columns')\n",
        "    X_train_test_valid_scaled_leaking = \\\n",
        "        pd.concat([X_train_test_valid_dates, \n",
        "                   X_train_test_valid_scaled_leaking], \n",
        "                  axis='columns')\n",
        "\n",
        "    X_train_scaled = X_train_test_scaled.iloc[:X_train.shape[0]]\n",
        "    X_test_scaled = X_train_test_scaled.iloc[X_train.shape[0]:]\n",
        "    X_valid_scaled = X_train_test_valid_scaled.iloc[X_train_test.shape[0]:]\n",
        "    X_valid_scaled_leaking = X_train_test_valid_scaled_leaking.iloc[X_train_test.shape[0]:]\n",
        "\n",
        "    return (train_test_scalers, \n",
        "            train_test_valid_scalers, \n",
        "            X_train_scaled, \n",
        "            X_test_scaled, \n",
        "            X_valid_scaled, \n",
        "            X_valid_scaled_leaking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_test_scalers, train_test_valid_scalers, X_train_scaled, X_test_scaled, X_valid_scaled, X_valid_scaled_leaking = \\\n",
        "    normalize_data(X_train, X_test, X_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write a reward scheme encouraging rare volatile upside trades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trade_flow.environments.default.rewards import TradeFlowRewardScheme\n",
        "\n",
        "\n",
        "class AnomalousProfit(TradeFlowRewardScheme):\n",
        "    \"\"\"A simple reward scheme that rewards the agent for exceeding a \n",
        "    precalculated percentage in the net worth.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    threshold : float\n",
        "        The minimum value to exceed in order to get the reward.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    threshold : float\n",
        "        The minimum value to exceed in order to get the reward.\n",
        "    \"\"\"\n",
        "\n",
        "    registered_name = \"anomalous\"\n",
        "\n",
        "    def __init__(self, threshold: float = 0.02, window_size: int = 1):\n",
        "        self._window_size = self.default('window_size', window_size)\n",
        "        self._threshold = self.default('threshold', threshold)\n",
        "\n",
        "    def get_reward(self, portfolio: 'Portfolio') -> float:\n",
        "        \"\"\"Rewards the agent for incremental increases in net worth over a\n",
        "        sliding window.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        portfolio : `Portfolio`\n",
        "            The portfolio being used by the environment.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            Whether the last percent change in net worth exceeds the predefined \n",
        "            `threshold`.\n",
        "        \"\"\"\n",
        "        performance = pd.DataFrame.from_dict(portfolio.performance).T\n",
        "        current_step = performance.shape[0]\n",
        "        if current_step > 1:\n",
        "            # Hint: make it cumulative.\n",
        "            net_worths = performance['net_worth']\n",
        "            ground_truths = precalculate_ground_truths(performance, \n",
        "                                                       column='net_worth', \n",
        "                                                       threshold=self._threshold)\n",
        "            reward_factor = 2.0 * ground_truths - 1.0\n",
        "            #return net_worths.iloc[-1] / net_worths.iloc[-min(current_step, self._window_size + 1)] - 1.0\n",
        "            return (reward_factor * net_worths.abs()).iloc[-1]\n",
        "\n",
        "        else:\n",
        "            return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PenalizedProfit(TradeFlowRewardScheme):\n",
        "    \"\"\"A reward scheme which penalizes net worth loss and \n",
        "    decays with the time spent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    cash_penalty_proportion : float\n",
        "        cash_penalty_proportion\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    cash_penalty_proportion : float\n",
        "        cash_penalty_proportion.\n",
        "    \"\"\"\n",
        "\n",
        "    registered_name = \"penalized\"\n",
        "\n",
        "    def __init__(self, cash_penalty_proportion: float = 0.10):\n",
        "        self._cash_penalty_proportion = \\\n",
        "            self.default('cash_penalty_proportion', \n",
        "                         cash_penalty_proportion)\n",
        "\n",
        "    def get_reward(self, portfolio: 'Portfolio') -> float:\n",
        "        \"\"\"Rewards the agent for gaining net worth while holding the asset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        portfolio : `Portfolio`\n",
        "            The portfolio being used by the environment.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            A penalized reward.\n",
        "        \"\"\"\n",
        "        performance = pd.DataFrame.from_dict(portfolio.performance).T\n",
        "        current_step = performance.shape[0]\n",
        "        if current_step > 1:\n",
        "            initial_amount = portfolio.initial_net_worth\n",
        "            net_worth = performance['net_worth'].iloc[-1]\n",
        "            cash_worth = performance['bitstamp:/USD:/total'].iloc[-1]\n",
        "            cash_penalty = max(0, (net_worth * self._cash_penalty_proportion - cash_worth))\n",
        "            net_worth -= cash_penalty\n",
        "            reward = (net_worth / initial_amount) - 1\n",
        "            reward /= current_step\n",
        "            return reward\n",
        "        else:\n",
        "            return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO: implement tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Trading Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import trade_flow.environments.default as default\n",
        "\n",
        "from trade_flow.agents.deprecated import TorchDQNAgent\n",
        "from trade_flow.feed.core import DataFeed, Stream\n",
        "from trade_flow.feed.core.base import NameSpace\n",
        "from trade_flow.environments.default.actions import BSH\n",
        "from trade_flow.environments.default.rewards import RiskAdjustedReturns, SimpleProfit\n",
        "from trade_flow.oms.exchanges import Exchange, ExchangeOptions\n",
        "from trade_flow.oms.services.execution.simulated import execute_order\n",
        "from trade_flow.oms.instruments import USD, BTC, ETH\n",
        "from trade_flow.oms.wallets import Wallet, Portfolio\n",
        "from trade_flow.oms.orders import TradeType\n",
        "\n",
        "# TODO: adjust according to your commission percentage, if present\n",
        "commission = 0.001\n",
        "price = Stream.source(list(X_train[\"close\"]), \n",
        "                      dtype=\"float\").rename(\"USD-BTC\")\n",
        "#bitstamp_options = ExchangeOptions(commission=commission)\n",
        "#bitstamp = Exchange(\"bitstamp\", \n",
        "#                    service=execute_order, \n",
        "#                    options=bitstamp_options)(price)\n",
        "bitstamp = Exchange(\"bitstamp\", \n",
        "                    service=execute_order)(price)\n",
        "\n",
        "cash = Wallet(bitstamp, 50000 * USD)\n",
        "asset = Wallet(bitstamp, 0 * BTC)\n",
        "\n",
        "portfolio = Portfolio(USD, [cash, asset])\n",
        "\n",
        "with NameSpace(\"bitstamp\"):\n",
        "    features = [\n",
        "        Stream.source(list(X_train_scaled[c]), \n",
        "                      dtype=\"float\").rename(c) for c in X_train_scaled.columns[1:]\n",
        "        #Stream.source(list(X_train_scaled['lr_close']), dtype=\"float\").rename('lr_close')\n",
        "    ]\n",
        "\n",
        "feed = DataFeed(features)\n",
        "feed.compile()\n",
        "\n",
        "renderer_feed = DataFeed([\n",
        "    Stream.source(list(X_train[\"date\"])).rename(\"date\"),\n",
        "    Stream.source(list(X_train[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
        "    Stream.source(list(X_train[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
        "    Stream.source(list(X_train[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
        "    Stream.source(list(X_train[\"close\"]), dtype=\"float\").rename(\"close\"), \n",
        "    Stream.source(list(X_train[\"volume\"]), dtype=\"float\").rename(\"volume\") \n",
        "])\n",
        "\n",
        "action_scheme = BSH(\n",
        "    cash=cash,\n",
        "    asset=asset\n",
        ")\n",
        "\n",
        "#reward_scheme = RiskAdjustedReturns(return_algorithm='sortino',\n",
        "#                                    window_size=30)\n",
        "\n",
        "#reward_scheme = SimpleProfit(window_size=30)\n",
        "\n",
        "reward_scheme = AnomalousProfit(threshold=threshold)\n",
        "\n",
        "#reward_scheme = PenalizedProfit(cash_penalty_proportion=0.1)\n",
        "\n",
        "env = default.create(\n",
        "    portfolio=portfolio,\n",
        "    action_scheme=action_scheme,\n",
        "    reward_scheme=reward_scheme,\n",
        "    feed=feed,\n",
        "    renderer_feed=renderer_feed,\n",
        "    renderer=default.renderers.PlotlyTradingChart(),\n",
        "    window_size=30\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.observer.feed.next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Train DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_optimal_batch_size(window_size=30, n_steps=1000, batch_factor=4, stride=1):\n",
        "    \"\"\"\n",
        "    lookback = 30          # Days of past data (also named window_size).\n",
        "    batch_factor = 4       # batch_size = (sample_size - lookback - stride) // batch_factor\n",
        "    stride = 1             # Time series shift into the future.\n",
        "    \"\"\"\n",
        "    lookback = window_size\n",
        "    sample_size = n_steps\n",
        "    batch_size = ((sample_size - lookback - stride) // batch_factor)\n",
        "    return batch_size\n",
        "\n",
        "batch_size = get_optimal_batch_size(window_size=window_size, n_steps=n_steps, batch_factor=4)\n",
        "batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(env)\n",
        "\n",
        "agent.train(batch_size=batch_size, \n",
        "            n_steps=n_steps, \n",
        "            n_episodes=n_episodes, \n",
        "            memory_capacity=memory_capacity, \n",
        "            save_path=save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implement validation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print basic quantstats report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_quantstats_full_report(env, data, output='dqn_quantstats'):\n",
        "    performance = pd.DataFrame.from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
        "    net_worth = performance['net_worth'].iloc[window_size:]\n",
        "    returns = net_worth.pct_change().iloc[1:]\n",
        "\n",
        "    # WARNING! The dates are fake and default parameters are used!\n",
        "    returns.index = pd.date_range(start=data['date'].iloc[0], freq='1d', periods=returns.size)\n",
        "\n",
        "    qs.reports.full(returns)\n",
        "    qs.reports.html(returns, output=output + '.html')\n",
        "\n",
        "print_quantstats_full_report(env, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
